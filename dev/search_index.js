var documenterSearchIndex = {"docs":
[{"location":"ODEmodels/#ODE-based-models","page":"ODE-based Models","title":"ODE-based models","text":"Often, an explicit analytical expression for a given mathematical model is not known. Instead, the model might be defined implicitly, e.g. as the solution to a system of ordinary differential equations. Especially in fields such as systems biology, modeling in terms of (various kinds of) differential equations is standard practice.\n\nAs a toy example, we will consider the well-known \"SIR model\" in the following, which groups a population into susceptible, infected and recovered subpopulations and describes the growths and decays of the respective populations via simple rate laws.\n\nWhile the DifferentialEquations.jl ecosystem offers many different ways of specifying such systems, we will use the syntax introduced by ModelingToolkit.jl since it is particularly convenient in this case.\n\nusing InformationGeometry, ModelingToolkit, Plots\n@parameters t β γ\n@variables S(t) I(t) R(t)\nDt = Differential(t)\n\nSIReqs = [ Dt(S) ~ -β * I * S,\n        Dt(I) ~ +β * I * S - γ * I,\n        Dt(R) ~ +γ * I]\n\nSIRstates = [S, I, R];    SIRparams = [β, γ]\n@named SIRsys = ODESystem(SIReqs, t, SIRstates, SIRparams)\n\nHere, the parameter β denotes the transmission rate of the disease and γ is the recovery rate. Note that in the symbolic scheme of ModelingToolkit.jl, the equal sign is represented via ~.\n\nAn infection dataset which is well-known in the literature is taken from an influenza outbreak at a English boarding school in 1978. Its numerical values can be found e.g. in table 1 of this paper. As no uncertainties associated with the number of infections is given, we will assume the 1sigma uncertainties to be pm 15 as a reasonable value. Further, it is known that the total number of students at said boarding school was 763 and we will therefore assume the initial conditions to be\n\nSIRinitial = [762, 1, 0.]\n\nfor the respective susceptible, infected and recovered subpopulations on day zero. Next, the DataSet object is constructed as:\n\ndays = collect(1:14)\ninfected = [3, 8, 28, 75, 221, 291, 255, 235, 190, 126, 70, 28, 12, 5]\nSIRDS = DataSet(days, infected, 15ones(14); xnames=[\"Days\"], ynames=[\"Infected\"])\n\nFinally, the DataModel associated with the SIR model and the given data is constructed by\n\nSIRobservables = [2]\nSIRDM = DataModel(SIRDS, SIRsys, SIRinitial, SIRobservables, [0.002, 0.5]; tol=1e-11)\n\nwhere SIRobservables denotes the components of the ODESystem that have actually been observed in the given dataset (i.e. the second component which are the infected in this case). The optional vector [0.001, 0.1] is our initial guess for the parameters [β, γ] for the maximum likelihood estimation and the keyword tol specifies the desired accuracy of the ODE solver for all model predictions.\n\ntip: Tip\nInstead of specifying the observable components of an ODE system as an array, it is also possible to provide an arbitrary observation function with argument signature f(u), f(u,t) or f(u,t,θ). Similarly, (parts of) the initial conditions for the ODE system can be included as parameters of the problem and estimated from data by providing a splitter function of the form θ -> (u0, p). The first entry of the returned tuple will be used as the initial condition for the ODE system and the second argument enters into the ODEFunction itself.In this particular example, one might include the initial number of infections as a dynamical parameter via the splitter function θ -> ([763.0 - θ[1], θ[1], 0.0], θ[2:3]).\n\nIt is now possible to compute properties of this DataModel such as confidence regions, confidence bands, geodesics, profile likelihoods, curvature tensors and so on as with any other model.\n\nsols = ConfidenceRegions(SIRDM, 1:2)\nVisualizeSols(SIRDM, sols)\n\nWhile it visually appears as though the confidence regions are perfectly ellipsoidal and the model would therefore be linearly dependent on its parameters β and γ, this is of course not the case. The non-linearity with respect to the parameters becomes much more apparent further away from the MLE, as one can confirm e.g. via radial geodesics emanating from the MLE or the profile likelihood.\n\nFinally the exact confidence bands associated with the computed confidence regions are obtained as:\n\nplot(SIRDM; Confnum=0)\nConfidenceBands(SIRDM, sols[2])\nplot(SIRDM; Confnum=0) # hide\nM = ConfidenceBands(SIRDM, sols[2]; plot=false) # hide\nInformationGeometry.PlotConfidenceBands(SIRDM, M; label=[\"$(round(InformationGeometry.GetConfnum(SIRDM,sols[2])))σ Conf. Band\" nothing]) # hide","category":"section"},{"location":"optimization/#Maximum-Likelihood-Estimation","page":"Maximum Likelihood Estimation","title":"Maximum Likelihood Estimation","text":"Unless the kwarg SkipOptim=true is passed to the DataModel constructor, it automatically tries to optimize the model parameters in an attempt to find the MLE. If no initial guess for the values parameters is provided, the constructor will try to infer the correct number of parameters and make a random initial guess. However, more control can be exerted over the optimization process by choosing an appropriate optimization method and corresponding options. These options may either be be passed to the DataModel constructor, or the optimization method InformationGeometry.minimize may be invoked explicitly after the DataModel was constructed with either unsuccessful optimization or skipping the automatic initial optimization altogether.\n\nMost importantly, InformationGeometry.minimize is compatible with the Optimization.jl ecosystem of optimizer methods via the meth keyword. Best support is available for optimizers from the Optim.jl via a custom wrapper. Choosing meth=nothing also allows for using a Levenberg-Marquardt method from the LsqFit.jl optimizer, which is however only possible if data with fixed Gaussian uncertainties and no priors are used.\n\nDM = DataModel(DataSet(1:3, [4,5,6.5], [0.5,0.45,0.6]), (x,p)->(p[1]+p[2])*x + exp(p[1]-p[2]), [1.3, 0.2]; SkipOptim=true)\nplot(DM; Confnum=0)\n\nThe optimization can then be performed with:\n\nusing Optim\nmle = InformationGeometry.minimize(DM; meth=NewtonTrustRegion(), tol=1e-12, maxtime=60.0, Domain=HyperCube(zeros(2), 10ones(2)), verbose=true)\n\nThe full solution object is returned if the keyword argument Full=true is additionally provided.\n\nAlternatively, one might use optimizers from the Optimization.jl ecosystem e.g. via\n\nusing Optimization, OptimizationOptimisers\nmle = InformationGeometry.minimize(DM, rand(2); meth=Optimisers.AdamW())\n\nor\n\nusing Optimization, OptimizationNLopt\nmle = InformationGeometry.minimize(DM, rand(2); meth=NLopt.GN_DIRECT())\n\nFinally, the newly found optimum can be visually inspected with\n\nplot(DM, mle)\n\nand added to the original unoptimized DataModel object via\n\nDM = DataModel(Data(DM), Predictor(DM), mle; SkipOptim=true)","category":"section"},{"location":"optimization/#Multistart-Optimization","page":"Maximum Likelihood Estimation","title":"Multistart Optimization","text":"When a reasonable estimate for the initial parameter configuration is not available and optimizations starting from the approximate center of the parameter domain do not converge to a suitable optimum, a systematic search of the parameter space is needed. This can be achieved with the MultistartFit method, which will sample the parameter space mathcalM either according to a given probability distribution (e.g. uniform / normal) on mathcalM or by drawing the parameter values from a low-discrepancy sequence such as the Sobol sequence. Compared with uniformly drawn values, values drawn from low-discrepancy sequences achieve a more even and \"equidistant\" coverage, thereby slightly increasing the chances of discovering a larger number of distinct local optima overall.\n\nJust like InformationGeometry.minimize, the MultistartFit method is also compatible with the Optimization.jl ecosystem of optimizer methods via the meth keyword.\n\nusing Optim\nR = MultistartFit(DM; N=200, maxval=500, meth=LBFGS())\n\nThe most relevant keywords are:\n\nMultistartDomain::HyperCube for defining the domain from which the initial guesses are drawn\nN::Int for choosing the number of starts\nmeth for choosing the optimizer\nresampling=false disables the drawing of new guesses when the objective function cannot be evaluated in some regions of the parameter space\nmaxval::Real=1e5 if no MultistartDomain is specified, a cube of size (-maxval, maxval)^(×n) is constructed\n\nThis returns a MultistartResults object, which saves some additional information such as the number of iterations taken per run, the initial guesses, etc. The parameter configuration is obtained with MLE(R). Alternatively, one could for example specify a distribution for the initials of the multistart optimization via\n\nMultistartFit(DM, MvNormal([0,0], Diagonal(ones(2))); MultistartDomain=HyperCube([-1,-1],[3,4]), N=200, meth=Newton())\n\nThe results of a multistart optimization can be visualized with a plot of the sorted final objective values, ideally showing prominent step-like features where the optimization reliably converged to the same local optima multiple times.\n\nWaterfallPlot(R; BiLog=true)\n\nIt can also be useful to investigate whether the parameter configurations within one \"step\" of the plot, where the final objective function value was the same, are actually \"close\" to each other, or whether there are distinct (or spread out) local optima, which nevertheless produce a fit of the same quality. This can be plotted via the ParameterPlot method, which can display the data either in a :dotplot, :boxplot or :violin plot. \n\nusing StatsPlots\nParameterPlot(R; st=:dotplot)\n\nSeeing a spread in the ParameterPlot for the parameter configurations of the lowest \"step\" is already an indication that some parameter may be non-identifiable.\n\nExemplary parameter configurations from the n-th step in a Waterfall plot can be conviently retrieved via GetStepParameters(R, n) for plotting and other analyses. For instance, to compare the fit corresponding to the local optimum constituted by the second step:\n\nplot(DM, GetStepParameters(R, 2))","category":"section"},{"location":"optimization/#InformationGeometry.minimize","page":"Maximum Likelihood Estimation","title":"InformationGeometry.minimize","text":"minimize(F::Function, start::AbstractVector{<:Number}; tol::Real=1e-10, meth=NelderMead(), Full::Bool=false, maxtime::Real=600, kwargs...) -> Vector\nminimize(F, dF, start::AbstractVector{<:Number}; tol::Real=1e-10, meth=LBFGS(;linesearch=LineSearches.BackTracking()), Full::Bool=false, maxtime::Real=600, kwargs...) -> Vector\nminimize(F, dF, ddF, start::AbstractVector{<:Number}; tol::Real=1e-10, meth=NewtonTrustRegion(), Full::Bool=false, maxtime::Real=600, kwargs...) -> Vector\n\nMinimizes the scalar input function using the given start using any algorithms from the Optimation.jl ecosystem specified via the keyword meth. Full=true returns the full solution object instead of only the minimizing result. Optionally, the search domain can be bounded by passing a suitable HyperCube object as the third argument (ignoring derivatives).\n\n\n\n\n\n","category":"function"},{"location":"optimization/#InformationGeometry.MultistartFit","page":"Maximum Likelihood Estimation","title":"InformationGeometry.MultistartFit","text":"MultistartFit(DM::AbstractDataModel; maxval::Real=1e3, MultistartDomain::HyperCube=FullDomain(pdim(DM), maxval), kwargs...)\n\nPerforms Multistart optimization with N starts and timeout of fits after timeout seconds. If resampling=true, if likelihood non-finite new initial starts are redrawn until N suitable initials are found.  If Robust=true, performs optimization wrt. p-norm according to given kwarg p. For Full=false, only the final MLE is returned, otherwise a MultistartResults object is returned, which can be further analyzed and plotted. The keyword TransformSample can be used to specify a function which is applied to the sample, allowing e.g. for sampling only a subset of the parameters and then adding on components which should stay at fixed initial values for the multistart.\n\nnote: Note\nAny further keyword arguments are passed through to the optimization procedure InformationGeometry.minimize such as tolerances, optimization methods, domain constraints, etc.\n\n\n\n\n\n","category":"function"},{"location":"optimization/#InformationGeometry.LocalMultistartFit","page":"Maximum Likelihood Estimation","title":"InformationGeometry.LocalMultistartFit","text":"LocalMultistartFit(DM::AbstractDataModel, Confnum::Real=2; kwargs...)\nLocalMultistartFit(DM::AbstractDataModel, mle::AbstractVector{<:Number}, Confnum::Real=2; kwargs...)\n\nPerforms a multistart search locally around a given MLE in a cube constructed from the Fisher information.\n\n\n\n\n\n","category":"function"},{"location":"optimization/#InformationGeometry.WaterfallPlot","page":"Maximum Likelihood Estimation","title":"InformationGeometry.WaterfallPlot","text":"WaterfallPlot(R::MultistartResults; BiLog::Bool=true, MaxValue::Real=3000, StepTol::Real=1e-3, kwargs...)\n\nShows Waterfall plot for the given results of MultistartFit. StepTol is used to decide which difference of two neighbouring values in the Waterfall plot constitutes a step. StepTol=0 deactivates step marks. MaxValue is used to set threshold for ignoring points whose cost function after optimization is too large compared with best optimum. BiLog=false disables logarithmic scale for cost function. A custom transformation can be specified via keyword Trafo::Function.\n\n\n\n\n\n","category":"function"},{"location":"optimization/#InformationGeometry.ParameterPlot-Tuple{MultistartResults}","page":"Maximum Likelihood Estimation","title":"InformationGeometry.ParameterPlot","text":"ParameterPlot(R::MultistartResults; st=:dotplot, BiLog::Bool=true, Nsteps::Int=5, StepTol::Real=1e-3, MaxValue=3000)\n\nPlots the parameter values of the MultistartResults separated by step to show whether the different optima are localized or not. st can be either :dotplot, :boxplot or :violin.\n\nnote: Note\nStatsPlots.jl needs to be loaded to use this plotting function.\n\n\n\n\n\n","category":"method"},{"location":"optimization/#InformationGeometry.IncrementalTimeSeriesFit-Tuple{DataModel}","page":"Maximum Likelihood Estimation","title":"InformationGeometry.IncrementalTimeSeriesFit","text":"IncrementalTimeSeriesFit(DM::AbstractDataModel, initial::AbstractVector{<:Number}=MLE(DM); steps::Int=length(Data(DM))÷5, Method::Function=InformationGeometry.minimize, kwargs...) -> Vector\n\nFits DataModel incrementally by splitting up the times series into chunks, e.g. fitting only the first quarter of data points, then half and so on. This can yield much better fitting results from random starting points, particularly for autocorrelated time series data. For example when the time series data oscillates in such a way that the optimization often gets stuck in a local optimum where the model fits a mostly straight line through the data, not correctly recognizing the oscillations.\n\n\n\n\n\n","category":"method"},{"location":"optimization/#InformationGeometry.RobustFit","page":"Maximum Likelihood Estimation","title":"InformationGeometry.RobustFit","text":"RobustFit(DM::AbstractDataModel, start::AbstractVector{<:Number}; tol::Real=1e-10, p::Real=1, kwargs...)\n\nUses p-Norm to judge distance on Dataspace as specified by the keyword.\n\n\n\n\n\n","category":"function"},{"location":"ProfileLikelihoodExt/#ProfileLikelihood.jl","page":"-","title":"ProfileLikelihood.jl","text":"Although profile likelihood functionality is also provided by InformationGeometry.jl, constructors for the types used by ProfileLikelihood.jl are also provided via an extension. This allows for straightforward comparisons of results between the methods provided by both packages. In particular, this also allows for computing two-dimensional profiles with bivariate_profile, where parameters are fixed in pairs at various values and the remaining parameters are re-optimized, as this functionality is not implemented by InformationGeometry.jl (yet?).\n\nFor computing the 1σ profiles, two possible methods are given by:\n\nusing InformationGeometry, ProfileLikelihood, Optimization, OptimizationOptimJL\nDM = DataModel(DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1]), (x::Real, θ::AbstractVector{<:Real}) -> θ[1]^3 * x + exp(θ[1] + θ[2]))\n\nprob = LikelihoodProblem(DM)\nsol = mle(prob, Optim.LBFGS())\nP1 = ProfileLikelihood.profile(prob, sol, 1:pdim(DM); alg=Optim.LBFGS(), parallel=true, conf_level=ConfVol(1),\n    threshold=-0.5InformationGeometry.InvChisqCDF(InformationGeometry.DOF(DM), ConfVol(1)), resolution=51)\n\n## Alternatively:\nP2 = ProfileLikelihood.profile(DM, 1; alg=Optim.LBFGS(), parallel=true, resolution=51)\n\nNote that the ProfileLikelihood.profile function is sensitive to the boundaries of the provided parameter domains. A HyperCube may be passed via the Domain kwarg, or a \n\nFor more examples of how to use the ProfileLikelihood.jl package, see the documentation.","category":"section"},{"location":"basics/#Introduction-to-Information-Geometry","page":"Basics of Information Geometry","title":"Introduction to Information Geometry","text":"For a more detailed discussion of information geometry, see e.g. my Master's Thesis, this paper, this other paper or this book.\n\nEssentially, information geometry is a combination of the mathematical disciplines of differential geometry and probability theory. The main idea is to rephrase statistical problems in such a way that they can be given a geometric interpretation.","category":"section"},{"location":"basics/#Information-Divergences","page":"Basics of Information Geometry","title":"Information Divergences","text":"In information theory, the dissimilarity between two probability distributions p(x) and q(x) is generally quantified using so-called information divergences, which are positive-definite functionals. The most popular choice of information divergence is given by the Kullback-Leibler divergence D_textKLpq defined by\n\nD_textKLpq coloneqq int mathrmd^m y  p(y)  mathrmln bigg( fracp(y)q(y) bigg) = mathbbE_p biggmathrmlnbigg( fracpq bigg) bigg\n\nIntuitively, the Kullback-Leibler divergence corresponds to the relative increase in Shannon entropy (i.e. loss of information) that is incurred by approximating the distribution p(x) through q(x). In addition to its tangible information-theoretic interpretation, the Kullback-Leibler divergence has the following desirable properties:\n\nreparametrization invariance with respect to the random variable over which the distributions are integrated,\napplicable between any two probability distributions with common support, e.g. a chi^2-distribution and a Poisson distribution or a normal and a Cauchy distribution.\n\nOn the other hand, the disadvantages of using information divergences such as the Kullback-Leibler divergence to measure the dissimilarity of distributions are:\n\nthey are typically not symmetric, i.e. D_textKLpq neq D_textKLqp\nthey usually do not satisfy a triangle inequality\n\nwherefore they do not constitute distance functions (i.e. metric functions) on the underlying space of probability distributions.","category":"section"},{"location":"basics/#The-Fisher-Metric","page":"Basics of Information Geometry","title":"The Fisher Metric","text":"In practical applications, one is often particularly interested in spaces of probability distributions which form a single overarching family and can be parametrized using a parameter configuration theta in mathcalM where mathcalM constitutes a smooth manifold. Accordingly, any two members p(ytheta_1) and p(ytheta_2) of this family can be compared using e.g. the Kullback-Leibler divergence D_textKLbigp(theta_1)p(theta_2)big via the formula given above.\n\nWhile the Kullback-Leibler divergence D_textKLpq does not constitute a proper distance function on mathcalM, it can be expanded in a Taylor series around theta_textMLE in terms of its derivatives. The zeroth order of this expansion vanishes due to the definiteness of the Kullback-Leibler divergence (i.e. D_textKLqq = 0 for all distributions q). Similarly, the first order vanishes since the expectation of the components of the score is nil. Thus, the second order approximation of the Kullback-Leibler divergence is completely determined by its Hessian, which can be computed as\n\ng_ab(theta) coloneqq biggfracpartial^2partial psi^a  partial psi^b  D_textKL bigp(ytheta)  p(ypsi) big bigg_psi = theta\n=  = -mathbbE_pbiggfracpartial^2  mathrmln(p)partial theta^a  partial theta^bbigg =  = mathbbE_pbiggfracpartial  mathrmln(p)partial theta^a fracpartial  mathrmln(p)partial theta^bbigg\n\nwhere it was assumed that the order of the derivative operator and the integration involved in the expectation value can be interchanged.\n\nThe Hessian of the Kullback-Leibler divergence is typically referred to as the Fisher information matrix. Moreover, since it can be shown that the Fisher information is not only positive-definite but also exhibits the transformation behaviour associated with a (02)-tensor field, it can therefore be used as a Riemannian metric on the parameter manifold mathcalM.\n\nClearly, the Riemannian geometry induced on mathcalM by the Fisher metric is ill-equipped to faithfully capture the behaviour of the Kullback-Leibler divergence in its entirety (e.g. its asymmetry). Nevertheless, this Riemannian approximation already encodes many of the key aspects of the Kullback-Leibler divergence and additionally benefits from the versatility and maturity of the differential-geometric formalism. Therefore, the Fisher metric offers a convenient and powerful tool which can be used to study statistical problems in a coordinate invariant setting which focuses on intrinsic properties of the parameter manifold.","category":"section"},{"location":"confidence-regions/#Confidence-Regions","page":"Confidence Regions","title":"Confidence Regions","text":"One of the primary goals of InformationGeometry.jl is to enable the user to investigate the relationships between different parameters in a model in detail by determining and visualizing the exact confidence regions associated with the best fit parameters. In this context, exact refers to the fact that no simplifying assumptions are made about the shape of the confidence regions.\n\nusing InformationGeometry, Plots\nDS = DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1])\nmodel(x::Real, θ::AbstractVector{<:Real}) = θ[1] * x + θ[2]\nDM = DataModel(DS, model)\n\nplot(DM; Confnum=0)\n\nDepending on how the parameters theta enter into the model, the shapes of confidence regions associated with the model may be distorted. For the linearly parametrized model y_textmodel(xtheta) = theta_1 cdot x + theta_2 from above, the 1 sigma and 2 sigma confidence regions form perfect ellipses around the maximum likelihood estimate as expected:\n\nsols = ConfidenceRegions(DM, 1:2; tol=1e-9)\nVisualizeSols(DM, sols)\n\nFor a non-linearly parametrized model such as y_textmodel(xtheta) = theta_1^3 cdot x + mathrmexp(theta_1 + theta_2) (which also produces a straight line fit!), the confidence regions are no longer ellipsoidal:\n\nmodel2(x::Real, θ::AbstractVector{<:Real}) = θ[1]^3 * x + exp(θ[1] + θ[2])\nDM2 = DataModel(DS, model2)\nsols2 = ConfidenceRegions(DM2, 1:2; tol=1e-9)\nVisualizeSols(DM2, sols2)\n\nSpecifically in the case of two-dimensional parameter spaces as shown here, the problem of finding the exact boundaries of the confidence regions is turned into a system of ordinary differential equations and subsequently solved using the DifferentialEquations.jl suite. As a result, the boundaries of the confidence regions are obtained in the form of ODESolution objects, which come equipped with elaborate interpolation methods.\n\nBoth finding as well as visualizing exact confidence regions for models depending on more than two parameters (i.e. mathrmdim  mathcalM  2) is more challenging from a technical perspective. For such models, it is clearly only possible to visualize three-dimensional slices of the parameter space at a time. The easiest way to achieve this is to intersect the confidence region with a family of 2D planes, in which the boundaries of the confidence region are computed using the 2D scheme.\n\nThe specific components of theta to be visualized can be passed as a tuple to ConfidenceRegion() via the keyword argument Dirs=(1,2,3). Also, the keyword N can be used to (approximately) control the number of planes with which the confidence region of interest is intersected.\n\nDM3 = DataModel(DS, (x,θ)-> θ[1]^3 * x + exp(θ[1] + θ[2]) + θ[3] * sin(x))\nPlanes, sols3 = ConfidenceRegion(DM3, 1; tol=1e-6, Dirs=(1,2,3), N=50)\nVisualizeSols(DM3, Planes, sols3)\n\nHere, only the 1sigma confidence region is shown. Given the non-linearity of the model, it is of course no surprise that the region is strongly distorted compared with a perfect ellipsoid.\n\nOnce the boundary of a confidence region associated with some particular level has been computed, it can be used to establish the most extreme deviations from the maximum likelihood prediction, which are possible at said confidence level. These can then be illustrated as so-called \"pointwise confidence bands\" around the best fit. For example, given the confidence boundaries of the model DM2 from above, the 2sigma confidence band can be obtained via:\n\nplot(DM2; Confnum=0)\nConfidenceBands(DM2, sols2[2])\nplot(DM2; Confnum=0) # hide\nM = ConfidenceBands(DM2, sols2[2]; plot=false) # hide\nInformationGeometry.PlotConfidenceBands(DM2, M; label=[\"$(round(InformationGeometry.GetConfnum(DM2,sols2[2])))σ Conf. Band\" nothing]) # hide","category":"section"},{"location":"confidence-regions/#InformationGeometry.ConfidenceRegions-Tuple{DataModel, Vector{Float64}}","page":"Confidence Regions","title":"InformationGeometry.ConfidenceRegions","text":"ConfidenceRegions(DM::DataModel, Range::AbstractVector)\n\nComputes the boundaries of confidence regions for two-dimensional parameter spaces given a vector or range of confidence levels. A convenient interface which extends this to higher dimensions is currently still under development.\n\nFor example,\n\nConfidenceRegions(DM, 1:3; tol=1e-9)\n\ncomputes the 1sigma, 2sigma and 3sigma confidence regions associated with a given DataModel using a solver tolerance of 10^-9.\n\nKeyword arguments:\n\nIsConfVol = true can be used to specify the desired confidence level directly in terms of a probability p in 01 instead of in units of standard deviations sigma,\ntol can be used to quantify the tolerance with which the ODE which defines the confidence boundary is solved (default tol = 1e-9),\nmeth can be used to specify the solver algorithm (default meth = Tsit5()),\nADmode=Val(false) computes the Score by separately evaluating the model as well as the Jacobian dmodel provided in DM. Other choices of ADmode directly compute the Score by differentiating the formula the log-likelihood, i.e. only one evaluation on a dual variable is performed.\nparallel = true parallelizes the computations of the separate confidence regions provided each process has access to the necessary objects,\ndof can be used to manually specify the degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"confidence-regions/#InformationGeometry.ConfidenceBands","page":"Confidence Regions","title":"InformationGeometry.ConfidenceBands","text":"ConfidenceBands(DM::DataModel, sol::AbstractODESolution, Xdomain::HyperCube; N::Int=300, plot::Bool=isloaded(:Plots)) -> Matrix\n\nGiven a confidence interval sol, the pointwise confidence band around the model prediction is computed for x values in Xdomain by evaluating the model on the boundary of the confidence region.\n\n\n\n\n\n","category":"function"},{"location":"PEtabExt/#PEtab.jl","page":"PEtab.jl","title":"PEtab.jl","text":"The PEtab.jl package allows for loading models defined in the PEtab standard for parameter estimation into Julia. In addition to datasets of recorded measurements, the PEtab format allows for the specification of a mathematical model for describing said data (which are usually based on Ordinary Differential Equations and provided via SBML files), as well as the final parameter values resulting from the estimation process. Most prominently, the PEtab standard has been used to publish modelling results in Systems Biology so far, see e.g. the Benchmark Collection.\n\nCurrently, it is possible to convert a PEtabODEProblem from the PEtab.jl package into a DataModel, (or a ConditionGrid if it consists of more than one condition) via by applying the DataModel constructor. For instance, for the Böhm model with .yaml saved under BöhmYamlPath:\n\nusing InformationGeometry, PEtab, Optim, Plots\nBöhm = PEtabODEProblem(PEtabModel(BöhmYamlPath); gradient_method=:ForwardEquations, hessian_method=:ForwardDiff)\nDM = Refit(DataModel(Böhm; FixedError=true); meth=IPNewton())\n\nThis will automatically extract a simplified representation of the dataset.  If error models are used in the PEtabODEProblem to estimate the data uncertainties, they are currently dropped and the uncertainties are fixed to the values dictated by the error model at the best fit values of the error parameters. However, since the likelihood function and its gradient are directly accessed from the given PEtabODEProblem, this does not affect optimisation (such as during profile likelihood computation or multistart optimisation), where changes in the given error parameters are properly accounted for.\n\nTherefore, this mainly affects plots of the datasets from the DM, if further changes to the values of the error parameters have been made after the import of the PEtabODEProblem to a DataModel or ConditionGrid, as these will currently not be visible in said plots of the dataset.\n\nMost other functionality of InformationGeometry.jl should be unaffected by this. In particular, it should be possible to compute ParameterProfiles, MultistartFits and so on without issue.\n\nplot(DM; Confnum=0)\nR = MultistartFit(DM; N=5000)\nP = ParameterProfiles(DM; N=50, meth=LBFGS(), Domain=nothing, ProfileDomain=InformationGeometry.Domain(DM))\n\nnote: Note\nFor PEtabODEProblems consisting of multiple conditions, only the gradient method :ForwardEquations is currently supported.   For the Hessian method, the three options :ForwardDiff, :BlockForwardDiff and :GaussNewton are supported.","category":"section"},{"location":"datamodels/#Providing-Datasets","page":"Providing Data and Models","title":"Providing Datasets","text":"Typically, one of the most difficult parts of any data science problem is to bring the data into a form which lends itself to the subsequent analysis. This section aims to describe the containers used by InformationGeometry.jl to store datasets and models in detail.\n\nThe data itself is stored using the DataSet container.\n\nTo complete the specification of an inference problem, a model function which is assumed to be able to capture the relationship which is inherent in the data must be added.\n\n\"Simple\" DataSets and DataModels can be visualized directly via plot(DM) using pre-written recipes for the Plots.jl package.\n\nA more structured container for models is ModelMap, which additionally stores information about custom parameter bounds, parameter names, arbitrary nonlinear constraints, etc.:","category":"section"},{"location":"datamodels/#InformationGeometry.DataSet","page":"Providing Data and Models","title":"InformationGeometry.DataSet","text":"The DataSet type is a versatile container for storing data. Typically, it is constructed by passing it three vectors x, y, sigma where the components of sigma quantify the standard deviation associated with each y-value. Alternatively, a full covariance matrix can be supplied for the ydata instead of a vector of standard deviations. The contents of a DataSet DS can later be accessed via xdata(DS), ydata(DS), ysigma(DS).\n\nExamples:\n\nIn the simplest case, where all data points are mutually independent and have a single x-component and a single y-component each, a DataSet consisting of four points can be constructed via\n\nDataSet([1,2,3,4], [4,5,6.5,7.8], [0.5,0.45,0.6,0.8])\n\nor alternatively by\n\nusing LinearAlgebra\nDataSet([1,2,3,4], [4,5,6.5,7.8], Diagonal([0.5,0.45,0.6,0.8].^2))\n\nwhere the diagonal covariance matrix in the second line is equivalent to the vector of standard deviations supplied in the first line.\n\nFor measurements with multiple components, it is also possible to enter them as a Matrix where the columns correspond to the respective components.\n\nDataSet([0, 0.5, 1], [1 100; 2 103; 3 108], [0.5 8; 0.4 5; 0.6 10])\n\nNote that if the uncertainty matrix is square, it may be falsely interpreted as a covariance matrix instead of as the columnwise specification of standard deviations.\n\nMore generally, if a dataset consists of N points where each x-value has n many components and each y-value has m many components, this can be specified to the DataSet constructor via a tuple (Nnm) in addition to the vectors x, y and the covariance matrix. For example:\n\nX = [0.9, 1.0, 1.1, 1.9, 2.0, 2.1, 2.9, 3.0, 3.1, 3.9, 4.0, 4.1]\nY = [1.0, 5.0, 4.0, 8.0, 9.0, 13.0, 16.0, 20.0]\nCov = Diagonal([2.0, 4.0, 2.0, 4.0, 2.0, 4.0, 2.0, 4.0])\ndims = (4, 3, 2)\nDS = DataSet(X, Y, Cov, dims)\n\nIn this case, X is a vector consisting of the concatenated x-values (with 3 components each) for 4 different data points. The values of Y are the corresponding concatenated y-values (with 2 components each) of said 4 data points. Clearly, the covariance matrix must therefore be a positive-definite (m cdot N) times (m cdot N) matrix.\n\n\n\n\n\n","category":"type"},{"location":"datamodels/#InformationGeometry.DataModel","page":"Providing Data and Models","title":"InformationGeometry.DataModel","text":"In addition to storing a DataSet, a DataModel also contains a function model(x,θ) and its derivative dmodel(x,θ) where x denotes the x-value of the data and θ is a vector of parameters on which the model depends. Crucially, dmodel contains the derivatives of the model with respect to the parameters θ, not the x-values. For example\n\nDS = DataSet([1,2,3,4], [4,5,6.5,7.8], [0.5,0.45,0.6,0.8])\nmodel(x::Number, θ::AbstractVector{<:Number}) = θ[1] * x + θ[2]\nDM = DataModel(DS, model)\n\nIn cases where the output of the model has more than one component (i.e. ydim > 1), it is advisable to define the model function in such a way that it outputs static vectors using StaticArrays.jl for increased performance. For ydim = 1, InformationGeometry.jl expects the model to output a number instead of a vector with one component. In contrast, the parameter configuration θ must always be supplied as a vector (even if it only has a single component).\n\nAn initial guess for the maximum likelihood parameters can optionally be passed to the DataModel as a vector via\n\nDM = DataModel(DS, model, [1.0,2.5])\n\nDuring the construction of a DataModel process which includes the search for the maximum likelihood estimate theta_textMLE, multiple tests are run. If necessary, the maximum likelihood estimation and subsequent tests are both skipped by appending true as the last argument in the constructor or by using the respective kwargs SkipTests=false or SkipOptim=false:\n\nDM = DataModel(DS, model, [-Inf,π,1], true)\n\nIf a DataModel is constructed as shown in the above examples, the gradient of the model with respect to the parameters θ (i.e. its \"Jacobian\") will be calculated using automatic differentiation. Alternatively, an explicit analytic expression for the Jacobian can be specified by hand:\n\nusing StaticArrays\nfunction dmodel(x::Number, θ::AbstractVector{<:Number})\n   @SMatrix [x  1.]     # ∂(model)/∂θ₁ and ∂(model)/∂θ₂\nend\nDM = DataModel(DS, model, dmodel)\n\nThe output of the Jacobian must be a matrix whose columns correspond to the partial derivatives with respect to different components of θ and whose rows correspond to evaluations at different components of x. Again, although it is not strictly required, outputting the Jacobian in form of a static matrix is typically beneficial for the overall performance.\n\nIt is also possible to specify a (logarithmized) prior distribution on the parameter space to the DataModel constructor after the initial guess for the MLE. For example:\n\nusing Distributions\nDist = MvNormal(ones(2), [1 0; 0 3.])\nLogPriorFn(θ) = logpdf(Dist, θ)\nDM = DataModel(DS, model, [1.0,2.5], LogPriorFn)\n\nThe DataSet contained in a DataModel named DM can be accessed via Data(DM), whereas the model and its Jacobian can be used via Predictor(DM) and dPredictor(DM) respectively. The MLE and the value of the log-likelihood at the MLE are accessible via MLE(DM) and LogLikeMLE(DM). The logarithmized prior can be accessed via LogPrior(DM).\n\n\n\n\n\n","category":"type"},{"location":"datamodels/#InformationGeometry.ModelMap","page":"Providing Data and Models","title":"InformationGeometry.ModelMap","text":"ModelMap(Map::Function, InDomain::Union{Nothing,Function}, Domain::HyperCube; startp::AbstractVector)\nModelMap(Map::Function, InDomain::Function, xyp::Tuple{Int,Int,Int})\n\nA container which stores additional information about a model map, in particular its domain of validity. Map is the actual map (x,θ) -> model(x,θ). Domain is a HyperCube which allows one to roughly specify the ranges of the various parameters. For more complicated boundary constraints, a function InDomain(θ) can be specified, for which all outputted components should be .≥ 0 on the valid parameter domain. Alternatively, InDomain may also be a bool-valued function, evaluating to true in admissible parts of the parameter domain.\n\nThe kwarg startp may be used to pass a suitable parameter vector for the ModelMap.\n\n\n\n\n\n","category":"type"},{"location":"ConditionGrids/#ConditionGrids","page":"Linking Multiple Models and Datasets","title":"ConditionGrids","text":"The ConditionGrid type offers a way to link a vector of DataModels, which in turn each contain a dataset and corresponding model to describe said data. Overall, the ConditionGrid takes a Vector of \"outer parameters\", i.e. a vector of parameters which are visible to the outside, and generates from these shared outer parameters new individual parameter vectors for each of the individual \"conditions\", i.e. DataModels. This terminology is inspired by the dMod package for dynamic modelling and parameter estimation in R.\n\nIn summary, a ConditionGrid requires at least the specification of a Vector of DataModels and a Vector of Functions, which perform the parameter transformations. For instance, for two datasets with different offset, they can be described by a shared slope parameter via:\n\nusing InformationGeometry\nDM1 = DataModel(DataSet(1:3,      [4,5,6.5], [0.5,0.45,0.6]), (x,p)->p[1].*x .+ p[2]; name=\"Condition 1\")\nDM2 = DataModel(DataSet(1:3, 5 .+ [4,5,6.5], [0.5,0.45,0.6]), (x,p)->p[1].*x .+ p[2]; name=\"Condition 2\")\nParameterTrafo = [ViewElements([1,2]), ViewElements([1,3])]\nCG = ConditionGrid([DM1, DM2], ParameterTrafo, rand(3); pnames=[\"Slope\", \"Offset_1\", \"Offset_2\"])\n\nnote: Note\nThe individual DataModels passed to ConditionGrid must have unique names. The name of a DataModel can be set by passing a String or Symbol via the keyword name to the constructor, or alternatively via remake(DM; name=\"CreativeConditionName\") given a DataModel object DM.\n\nOptionally, it is possible to specify an additional prior for the outer parameters (in addition to the possible individual priors of the contained DataModels), a parameter domain for the outer parameters and custom parameter names via keyword arguments to the ConditionGrid constructor. To the outside, the ConditionGrid behaves like a single DataModel in terms of plotting, evaluating functions like the likelihood, optimization and computation of parameter profiles. The individual conditions can be accessed via Conditions(CG).\n\nnote: Note\nIf optimization is performed on a ConditionGrid, the resulting MLE is not propagated to the individual conditions. Therefore MLE(Conditions(CG)[i]) will return the original MLEs of the individual DataModels. The individual MLEs based on the current vector of outer parameters can be computed via CG.Trafos[i](MLE(CG)).\n\nIf no vector of parameter transformations is specified, the default vector of parameter transformations will keep the individual model parameters separate such that the outer parameters are given by the vertical concatenation of the parameter configurations of the individual conditions. \n\nPredictions for individual conditions at x-values xran can be generated by passing the condition name as a Symbol as the last positional argument to EmbeddingMap or EmbeddingMatrix:\n\nxran = range(0, 4; length=151)\nY = EmbeddingMap(CG, MLE(CG), xran, Symbol(\"Condition 1\"))","category":"section"},{"location":"ConditionGrids/#InformationGeometry.ConditionGrid","page":"Linking Multiple Models and Datasets","title":"InformationGeometry.ConditionGrid","text":"ConditionGrid(DMs::AbstractVector{<:AbstractDataModel}, Trafos::AbstractVector{<:Function}, mle::AbstractVector, LogPriorFn::Union{Nothing,Function}=nothing; Domain::Union{Nothing,Cuboid}=nothing, \n                SkipOptim::Bool=false, pnames::AbstractVector{<:StringOrSymb}=CreateSymbolNames(length(mle)), name::StringOrSymb=\"\")\n\nImplements condition grid inspired by R package dMod. Connects different given DataModels via a vector of parameter transformations, which read from the same collective vector of outer parameter values and compute from them the individual parameter configurations of the respective DataModels from this at every step. Thus, this allows for easily connecting different datasets with distinct models while performing simultaneous inference with shared parameters between the models.\n\n\n\n\n\n","category":"type"},{"location":"transformations/#Model-Transformations","page":"Model Transformations","title":"Model Transformations","text":"Occasionally, one might wish to perform coordinate transformations on the parameter space of a model without having to redefine the entire model as this can be a cumbersome process for complex models. For example, this might be useful in the fitting process when the allowable parameter range spans several orders of magnitude or when trying to enforce the positivity of parameters.\n\nA few methods are provided to make this process more convenient. These include: LogTransform, ExpTransform, Log10Transform, Exp10Transform, ReflectionTransform and ScaleTransform. These methods accept a vector of booleans as an optional second argument to restrict the application of the transformation to specific parameter components if desired. For first argument, one can either provide just a model function to obtain its transformed counterpart or alternatively supply an entire DataModel to be transformed.\n\nusing InformationGeometry # hide\nDM = DataModel(DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1]), LinearModel)\nlogDM = LogTransform(DM)\nExpDM = ExpTransform(Exp10Transform(DM, [false, true]), [true, false])\nSymbolicModel(logDM), SymbolicModel(ExpDM)\n\nIt is also possible to provide other differentiable functions for parameter transformations by hand using the following method:\n\nThe provided scalar function F should be strictly monotonic to avoid problems when differentiating the model.\n\nIn addition to componentwise application of scalar functions to the parameters, there are also higher-dimensional transformations such as TranslationTransform, LinearTransform and their combination AffineTransform which allow for mixing between the components.\n\nLastly, the method LinearDecorrelation is a special case of AffineTransform which subtracts the MLE from the parameters and applies the cholesky decomposition (i.e. \"square root\") of the inverse Fisher metric at the best fit. This centers the confidence regions on the origin and will result in confidence boundaries which constitute concentric circles / spheres for linearly parametrized models. For models which are non-linear with respect to their parameters, the confidence boundaries of the \"linearly decorrelated\" model showcase the deviations of the confidence boundaries of the original model from ellipsoidal shape, therefore nicely illustrating the magnitude of the coordinate distortion present on the parameter space.\n\nFor general (differentiable) multivariable transformations on the parameter space, one can use:","category":"section"},{"location":"transformations/#InformationGeometry.ComponentwiseModelTransform","page":"Model Transformations","title":"InformationGeometry.ComponentwiseModelTransform","text":"ComponentwiseModelTransform(DM::AbstractDataModel, F::Function, idxs=trues(pdim(DM))) -> DataModel\nComponentwiseModelTransform(model::Function, idxs, F::Function) -> Function\n\nTransforms the parameters of the model by the given scalar function F such that newmodel(x, θ) = oldmodel(x, F.(θ)). By providing idxs, one may restrict the application of the function F to broadcast only to specific parameter components.\n\nFor vector-valued transformations, see ModelEmbedding.\n\n\n\n\n\n","category":"function"},{"location":"transformations/#InformationGeometry.ModelEmbedding","page":"Model Transformations","title":"InformationGeometry.ModelEmbedding","text":"ModelEmbedding(DM::AbstractDataModel, F::Function, start::AbstractVector; Domain::HyperCube=FullDomain(length(start),Inf)) -> DataModel\n\nTransforms a model function via newmodel(x, θ) = oldmodel(x, F(θ)) and returns the associated DataModel. An initial parameter configuration start as well as a Domain can optionally be passed to the DataModel constructor.\n\nFor component-wise transformations see ComponentwiseModelTransform.\n\n\n\n\n\n","category":"function"},{"location":"parameter-profiles/#Profile-Likelihoods","page":"Profile Likelihoods","title":"Profile Likelihoods","text":"Whenever the parameter space of a non-linearly parametrized model is high-dimensional, computing exact confidence regions via integral curves becomes increasingly difficult and time intensive. Moreover, it becomes more likely that not all the parameters are sufficiently informed by the available data for the confidence regions of levels 1 sigma or 2 sigma to be bounded. Therefore, a popular alternative for the individual uncertainty assessments of the model parameters theta in mathcalM is given by the profile likelihood.\n\nIn essence, the profile likelihood simply constitutes a projection of the high-dimensional simultaneous confidence region onto the individual parameter directions. As a result, although there may be directions in which a given confidence region is not bounded since the associated parameter component is insufficiently informed by the data, the remaining directions in which the confidence region is bounded are unaffected. Thus, the one-dimensional profile likelihoods provide a convenient and exact summary of the individual uncertainties associated with the model parameters.\n\nFor an in-depth discussion of the theory underlying the profile likelihood, see e.g. this article.\n\nReturning again to the example from the previous section on Confidence Regions, we define:\n\nusing InformationGeometry, Plots\nDS = DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1])\nmodel(x::Real, θ::AbstractVector{<:Real}) = θ[1] * x + θ[2]\nDM = DataModel(DS, model)\nmodel2(x::Real, θ::AbstractVector{<:Real}) = θ[1]^3 * x + exp(θ[1] + θ[2])\nDM2 = DataModel(DS, model2)\n\nInstead of computing the associated confidence regions as before, we can compute the profile likelihoods via\n\nP1 = ParameterProfiles(DM, 2; N=100, plot=true, IsCost=false, adaptive=true, SaveTrajectories=true)\nplot(P1, false) #hide\n\nwhere the second argument 2 determines the confidence level in units of sigma to which the profile is computed and keyword N determines the number of points in the profile (approximately).  The keyword IsCost=false automatically applies a rescaling to the vertical axis via the inverse chi^2 distribution with mathrmdimmathcalM degrees of freedom so that it already displays the confidence level up to which the parameter value on the horizontal axis is still compatible with the data.  In contrast, the default IsCost=true does not apply this rescaling and provides the cost function value 2big(ell(theta_MLE) - mathrmPL_i(theta_i)big) with ell the log-likelihood and mathrmPL_i(theta_i) the profile likelihood of the i-th parameter.\n\nSince the model function of DM is linear with respect to all its parameters all its profiles are not only symmetric around the MLE, but also bounded up to arbitrary confidence levels. In comparison, the non-linearly parametrized model map from DM2 results in asymmetric profiles:\n\nP2 = ParameterProfiles(DM2, 2; N=100, plot=true, IsCost=false, adaptive=true, SaveTrajectories=true)\nplot(P2, false) #hide\n\nMoreover, when computing the profile likelihood for even higher confidence levels of up to 3sigma, we finally see that the profile of the second parameter theta_2 does not reach above the 3sigma threshold, and is \"open\" to one side:\n\nP3 = ParameterProfiles(DM2, 3; N=100, plot=true, IsCost=false, adaptive=true, SaveTrajectories=true)\nplot(P3, false) #hide\n\nThis means that arbitrarily small values of theta_2 are still compatible with the data up to a confidence of 3sigma approx 9973.\n\nOnce a profile P3 has been computed, it can be plotted via plot(P3). It is also possible to just plot any of the individual profiles via plot(P3[i]) where the integer i denotes the profile of the i-th parameter. ","category":"section"},{"location":"parameter-profiles/#Extracting-Confidence-Intervals","page":"Profile Likelihoods","title":"Extracting Confidence Intervals","text":"Finally, the precise values where the profile likelihood of a given parameter intersects the threshold corresponding to some confidence level 1-alpha = q in (01) can be extracted from a ParameterProfiles object via\n\nProfileBox(P3, 1)\n\nwhich computes the 1sigma confidence intervals as a HyperCube where the confidence level is again provided in units of sigma. The desired confidence level can also be provided in  via InvConfVol. For instance, to compute precisely the intervals associated with the 95 approx 1959sigma thresholds:\n\nTuple(ProfileBox(P3, InvConfVol(0.95)))\n\nand the confidence intervals can be extracted from the returned HyperCube by indexing into the result or applying Tuple. When the confidence interval of associated with a given level is not bounded to one or both sides, -Inf or +Inf respectively is returned. For instance, the half-open 3sigma interval for theta_2:\n\nProfileBox(P3[2], 3)[1]\n\nThe final confidence level up to which a parameter was identified by the available data, i.e. where the confidence interval is still bounded in both directions, can be computed more precisely from a given profile by\n\nPracticallyIdentifiable(P3[2])\n\nwhich can either be computed for all profiles together or a single profile of interest.","category":"section"},{"location":"parameter-profiles/#Investigating-Profile-Paths","page":"Profile Likelihoods","title":"Investigating Profile Paths","text":"The paths traced out by the individual profiles in the parameter space encode useful information about the mutual relationships between different parameters. For instance, if during the profile of a given parameter a, the nuisance parameter b remains at a constant value throughout, this indicates that the effects of a are independent of the mechanism encoded by b. In this sense, the influences of the two respective mechanisms on the final model predictions can be considered to be orthogonal. Conversely, if the nuisance parameter b changes at a somewhat linear or even superlinear rate along the profile of a, this means that the effects of the two mechanisms are strongly linked in the sense that changes in the value of the nuisance parameter b are able to compensate at least partially for changes in the parameter a. Moreover, the directionality of the changes in b reveals whether the mutual effects of the respective mechanisms are synergistic or antagonistic. This kind of nuanced information is crucial in the process of model reduction.\n\nFor example, in ODE-based mechanistic models describing biological systems, this can be exploited to perform systematic model reduction in a straightforward, iterative process, where practically non-identifiable parameters are successively removed, by fixing them to their limiting values of zero or infinity, until all remaining parameters of the model are fully identified by the given data. A paper outlining this systematic reduction strategy in detail is given by Driving the model to its limits.\n\nThe saved paths of a ParameterProfiles object P can be accessed via InformationGeometry.Trajectories(P) for computations. They can be plotted via several methods, such as PlotProfilePaths:\n\nPlotProfilePaths(P3; RelChange=false, TrafoPath=identity, idxs=1:length(P3))\n\nwhere the RelChange kwarg can be used to specify whether the relative change p_i / p_mle or the difference p_i - p_mle is plotted. Also, the TrafoPath allows for specifying a transformation which is broadcasted over the results to account for non-linear distortions on the parameter space, making relevant features more easily visible, for instance TrafoPath=BiLog.\n\nIn the above example, one can see from the trajectories of the profile for theta_2 that the first parameter theta_1 must decrease if the second parameter theta_2 increases in order to compensate, such that the model still describes the data as well as possible.\n\nIt is also possible to plot the finite differences of the profile paths (i.e. their discrete first derivative) via PlotProfilePathDiffs and PlotProfilePathNormDiffs, which can make it easier to visually identify discontinuous jumps in the parameter paths.","category":"section"},{"location":"parameter-profiles/#InformationGeometry.ProfileBox-Tuple{ParameterProfiles}","page":"Profile Likelihoods","title":"InformationGeometry.ProfileBox","text":"ProfileBox(P::ParameterProfiles, Confnum::Real=1; Interp=DataInterpolations.QuadraticInterpolation, kwargs...)\n\nConstructs HyperCube which bounds the confidence region associated with the confidence level Confnum from the interpolated likelihood profiles.\n\n\n\n\n\n","category":"method"},{"location":"parameter-profiles/#InformationGeometry.PracticallyIdentifiable-Tuple{ParameterProfiles}","page":"Profile Likelihoods","title":"InformationGeometry.PracticallyIdentifiable","text":"PracticallyIdentifiable(P::ParameterProfiles) -> Real\n\nDetermines the maximum level at which ALL the given profiles in ParameterProfiles are still practically identifiable. If IsCost=true was chosen for the profiles, the output is the maximal deviation in cost function value W = 2(L_MLE - PL_i(θ)). If instead IsCost=false was chosen, so that cost function deviations have already been rescaled to confidence levels, the output of PracticallyIdentifiable is the maximal confidence level in units of standard deviations σ where the model is still practically identifiability.\n\n\n\n\n\n","category":"method"},{"location":"kullback-leibler/#Kullback-Leibler-Divergences","page":"-","title":"Kullback-Leibler Divergences","text":"Using the Distributions type provided by Distributions.jl, the KullbackLeibler method offers a convenient way of computing the Kullback-Leibler divergence between distributions. In several cases an analytical expression for the Kullback-Leibler divergence is known. These include: (univariate and multivariate) Normal, Cauchy, Exponential, Weibull and Gamma distributions.\n\nFurthermore, for distributions over a one-dimensional domain where no analytic result is known, KullbackLeibler rephrases the integral in terms of an ODE and employs an efficient integration scheme from the DifferentialEquations.jl suite. For multivariate distributions, Monte Carlo integration is used.\n\nExamples of use:\n\nKullbackLeibler(Cauchy(1.,2.4), Normal(-4,0.5), HyperCube([-100,100]); tol=1e-12)\nKullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-50,50],[-50,50]]); Carlo=true, N=Int(1e8))\n\nIn addition, it is of course also possible to input generic functions, whose positivity and normalization should be ensured by the user.\n\nFor example, the Kullback-Leibler divergence between a Cauchy distribution with mu=1 and s=2 and a normal (i.e. Gaussian) distribution with mu=-4 and sigma=12 can be calculated via:\n\nusing InformationGeometry # hide\nusing LinearAlgebra, Distributions\nKullbackLeibler(Cauchy(1.,2.), Normal(-4.,0.5), HyperCube([-100,100]); tol=1e-12)\n\nSpecifically, the keyword arguments used here numerically compute the divergence over the domain -100100 to a tolerance of approx 10^-12.\n\nThe domain of the integral involved in the computation of the divergence is specified using the HyperCube datatype, which stores a cuboid region in N dimensions as a vector of intervals.\n\nFurthermore, the Kullback-Leibler divergence between multivariate distributions can be computed for example by\n\nKullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-20,50],[-20,50]]); tol=1e-8)\n\nusing adaptive integration methods from HCubature.jl. Alternatively, Monte Carlo integration can be employed by specifying Carlo=true:\n\nKullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-50,50],[-50,50]]); Carlo=true, N=Int(5e6))\n\nIn addition, the keyword argument N now determines the number of points where the integrand is evaluated over the given domain -5050 times -5050.\n\nSo far, importance sampling has not been implemented for the Monte Carlo integration. Instead, the domain is sampled uniformly.","category":"section"},{"location":"kullback-leibler/#InformationGeometry.KullbackLeibler-Tuple{Function, Function, HyperCube}","page":"-","title":"InformationGeometry.KullbackLeibler","text":"KullbackLeibler(p::Function,q::Function,Domain::HyperCube=HyperCube([-15,15]); tol=2e-15, N::Int=Int(3e7), Carlo::Bool=(length(Domain)!=1))\n\nComputes the Kullback-Leibler divergence between two probability distributions p and q over the Domain. If Carlo=true, this is done using a Monte Carlo Simulation with N samples. If the Domain is one-dimensional, the calculation is performed without Monte Carlo to a tolerance of ≈ tol.\n\nD_textKLpq coloneqq int mathrmd^m y  p(y)  mathrmln bigg( fracp(y)q(y) bigg)\n\n\n\n\n\n","category":"method"},{"location":"kullback-leibler/#InformationGeometry.HyperCube","page":"-","title":"InformationGeometry.HyperCube","text":"The HyperCube type is used to specify a cuboid region in the form of a cartesian product of N real intervals, thereby offering a convenient way of passing domains for integration or plotting between functions. A HyperCube object cube type has two fields: cube.L and cube.U which are two vectors which respectively store the lower and upper boundaries of the real intervals in order. Examples for constructing HyperCubes:\n\nHyperCube([[1,3],[π,2π],[-500,100]])\nHyperCube([1,π,-500],[3,2π,100])\nHyperCube([[-1,1]])\nHyperCube([-1,1])\nHyperCube(collect([-7,7.] for i in 1:3))\n\nExamples of quantities that can be computed from and operations involving a HyperCube object X:\n\nCubeVol(X)\nTranslateCube(X,v::AbstractVector)\nCubeWidths(X)\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#Advanced-Datasets","page":"Advanced Dataset Types","title":"Advanced Datasets","text":"Depending on the nature of the dataset that is analyzed, there are multiple data types implemented by InformationGeometry.jl to store them in. Mainly, these data types provide a trade-off in speed / simplicity versus flexibility / generality as illustrated by the following table:\n\nContainer allows non-Gaussian y-uncertainty allows missing values allows x-uncertainty allows mixed x-y uncertainty allows y-uncertainty estimation allows x-uncertainty estimation\nDataSet ❌ ❌ ❌ ❌ ❌ ❌\nDataSetExact ✅ ❌ ✅ ❌ ❌ ❌\nCompositeDataSet ✅ ✅ ❌ ❌ ❌ ❌\nGeneralizedDataSet ✅ ❌ ✅ ✅ ❌ ❌\nDataSetUncertain ❌ ❌ ❌ ❌ ✅ ❌\nUnknownVarianceDataSet ❌ ❌ ❌ ❌ ✅ ✅","category":"section"},{"location":"AdvancedData/#InformationGeometry.DataSetExact","page":"Advanced Dataset Types","title":"InformationGeometry.DataSetExact","text":"DataSetExact(x::AbstractArray, y::AbstractArray, Σ_y::AbstractArray)\nDataSetExact(x::AbstractArray, Σ_x::AbstractArray, y::AbstractArray, Σ_y::AbstractArray)\nDataSetExact(xd::Distribution, yd::Distribution, dims::Tuple{Int,Int,Int}=(length(xd),1,1))\n\nA data container which allows for uncertainties in the independent variables, i.e. x-variables. Moreover, the observed data is stored in terms of two probability distributions over the spaces mathcalX^N and mathcalY^N respectively, which also allows for uncertainties in the observations that are non-Gaussian. For instance, the uncertainties associated with a given observation might follow a Cauchy, t-student, log-normal or some other smooth distribution.\n\nExamples:\n\nusing InformationGeometry, Distributions\nX = product_distribution([Normal(0, 1), Cauchy(2, 0.5)])\nY = MvTDist(2, [3, 8.], [1 0.5; 0.5 3])\nDataSetExact(X, Y, (2,1,1))\n\nnote: Note\nUncertainties in the independent x-variables are optional for DataSetExact, and can be set to zero by wrapping the x-data in a InformationGeometry.Dirac \"distribution\". The following illustrates numerically equivalent ways of encoding a dataset whose uncertainties in the x-variables is zero:using InformationGeometry, Distributions, LinearAlgebra\nDS1 = DataSetExact(InformationGeometry.Dirac([1,2]), MvNormal([5,6], Diagonal([0.1, 0.2].^2)))\nDS2 = DataSetExact([1,2], [5,6], [0.1, 0.2])\nDS3 = DataSet([1,2], [5,6], [0.1, 0.2])where DS1 == DS2 == DS3 will evaluate to true.\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#InformationGeometry.CompositeDataSet","page":"Advanced Dataset Types","title":"InformationGeometry.CompositeDataSet","text":"The CompositeDataSet type is a more elaborate (and typically less performant) container for storing data. Essentially, it splits observed data which has multiple y-components into separate data containers (e.g. of type DataSet), each of which corresponds to one of the components of the y-data. Crucially, each of the smaller data containers still shares the same \"kind\" of x-data, that is, the same xdim, units and so on, although they do not need to share the exact same particular x-data.\n\nThe main advantage of this approach is that it can be applied when there are missing y-components in some observations. A typical use case for CompositeDataSets are time series where multiple quantities are tracked but not every quantity is necessarily recorded at each time step. Example:\n\nusing DataFrames\nt = [1,2,3,4]\ny₁ = [2.5, 6, missing, 9];      y₂ = [missing, 5, 3.1, 1.4]\nσ₁ = 0.3*ones(4);               σ₂ = [missing, 0.2, 0.1, 0.5]\ndf = DataFrame([t y₁ σ₁ y₂ σ₂], :auto)\n\nxdim = 1;   ydim = 2\nCompositeDataSet(df, xdim, ydim; xerrs=false, stripedYs=true)\n\nThe boolean-valued keywords stripedXs and stripedYs can be used to indicate to the constructor whether the values and corresponding 1sigma uncertainties are given in alternating order, or whether the initial block of ydim many columns are the values and the second ydim many columns are the corresponding uncertainties. Also, xerrs=true can be used to indicate that the x-values also carry uncertainties. Basically all functions which can be called on other data containers such as DataSet have been specialized to also work with CompositeDataSets.\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#InformationGeometry.GeneralizedDataSet","page":"Advanced Dataset Types","title":"InformationGeometry.GeneralizedDataSet","text":"GeneralizedDataSet(dist::ContinuousMultivariateDistribution, dims::Tuple{Int,Int,Int}=(length(dist), 1, 1))\n\nData structure which can take general x-y-covariance into account where dims=(Npoints, xdim, ydim) indicates the dimensionality of the data. dist should constitute a smooth distribution over the space mathcalX^N times mathcalY^N where mean(dist) is interpreted as the concatenation of the (most likely values for the) observations (x_1  x_N y_1  y_N) and the width of dist specifies the uncertainty in the signal. Typically, dist is a multivariate Gaussian but other distributions such as Cauchy or student's t-distributions are also possible. Thus, arbitrary correlations between the dependent y and independent x variables can be encoded.\n\nnote: Note\nIf there is no correlation between the x and y variables (i.e. if the offdiagonal blocks of cov(dist) are zero), it can be more performant to use the type DataSetExact to encode the given data instead.\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#InformationGeometry.DataSetUncertain","page":"Advanced Dataset Types","title":"InformationGeometry.DataSetUncertain","text":"DataSetUncertain(x::AbstractVector, y::AbstractVector, σ⁻¹::Function, c::AbstractVector; BesselCorrection::Bool=false)\nDataSetUncertain(x::AbstractVector, y::AbstractVector, σ⁻¹::Function, errorparamsplitter::Function, c::AbstractVector, dims::Tuple{Int,Int,Int}; BesselCorrection::Bool=false)\n\nThe DataSetUncertain type encodes data for which the size of the variance is unknown a-priori but whose error is specified via an error model of the form σ(x, y_pred, c) where c is a vector of error parameters. This parametrized error model is subsequently used to estimate the standard deviations in the observations y.\n\nnote: Note\nTo enhance performance, the implementation actually requires the specification of a reciprocal error model, i.e. a function σ⁻¹(x, y_pred, c). If ydim is larger than one, the reciprocal error model should output a matrix, i.e. the cholesky decomposition S of the covariance Σ such that Σ == S' * S.\n\nTo construct a DataSetUncertain, one has to specify a vector of independent variables x, a vector of dependent variables y, a reciprocal error model σ⁻¹(x, y_pred, c) and an initial guess for the vector of error parameters c. Optionally, an explicit errorparamsplitter function of the form θ -> (modelparams, errorparams) may be specified, which splits the parameters into a tuple of model parameters, which are subsequently forwarded into the model, and error parameters c, which are only passed to the reciprocal error model σ⁻¹.\n\nwarn: Warn\nThe parameters which are visible to the outside are processed by errorparamsplitter FIRST, before forwarding into the model, where modelparams might be further modified by embedding transformations.\n\nExamples:\n\nIn the simplest case, where all data points are mutually independent and have a single x-component and a single y-component each, a DataSet consisting of four points can be constructed via\n\nDS = DataSetUncertain([1,2,3,4], [4,5,6.5,7.8], (x,y,c)->1/exp10(c[1]), [0.5])\n\nnote: Note\nIt is generally advisable to exponentiate error parameters, since they are penalized poportional to log(c) in the normalization term of Gaussian likelihoods. A Bessel correction sqrt((length(ydata(DS))-length(params))/length(ydata(DS))) can be applied to the reciprocal error to account for the fact that the maximum likelihood estimator for the variance is biased via kwarg BesselCorrection.\n\n\n\n\n\n","category":"type"},{"location":"parallelization/#Parallization","page":"Parallelization","title":"Parallization","text":"Especially for cases where every single evaluation of the likelihood is computationally expensive (e.g. because the model function is highly complex and / or has to be evaluated for a very large number of data points) a lot of performance can be gained by distributing the workload between multiple threads.\n\nEarly on in the development of this package, the design choice was made that computations of quantities such as the likelihood should be kept local to avoid unnecessary overhead for likelihoods which are cheap. However, computations of multiple trajectories on the confidence boundary can be evaluated in parallel.\n\nA prerequisite for parallel computation is that every process has access to the necessary DataModel objects. For example, this can be achieved using the @everywhere macro from Distributed.jl. Note that in this case every step involved in the definition of the DataModel, its DataSet and model function must be performed on each worker simultaneously, e.g. by wrapping all loading and construction steps in an @everywhere begin ... end environment.\n\nAlternatively, it is also possible to share data between processes using packages such as ParallelDataTransfer.jl. Here, only the final DataModel needs to be sent to other workers instead of having to perform all intermediate steps (such as the maximum likelihood estimation involved in the DataModel construction) on each worker.\n\nBoth the functions ConfidenceRegion() and ConfidenceRegions() accept the optional keyword parallel=true to enable parallel computations of confidence boundaries. Other methods which also accept the keyword parallel=true include PlotScalar(), ParameterProfiles() and RadialGeodesics().\n\nExample:\n\nusing Distributed;  addprocs(4)\n@everywhere using InformationGeometry, ParallelDataTransfer\nusing Distributions, Random, BenchmarkTools\n\nRandom.seed!(123)\nX = collect(1:300);     Y = 0.02*X.^2 - 5*X .+ 10 + rand(Normal(0,5),300)\n\nDS = DataSet(X, Y, 5 .* ones(300) + 2rand(300))\nDM = DataModel(DS, (x,θ)->sinh(θ[1])*x^2 + (θ[2]+θ[3])*x + (θ[2]-θ[3]))\n\nsendto(workers(); DM=DM)\n\n@btime ConfidenceRegion(DM, 1; parallel=true, tests=false)\n@btime ConfidenceRegion(DM, 1; parallel=false, tests=false)","category":"section"},{"location":"todo/#Contributing","page":"Contributing","title":"Contributing","text":"If you encounter a bug, feel free to file an issue detailing the problem in contrast to the behaviour you were expecting. Please provide a minimal working example and make sure to specify the particular version of InformationGeometry.jl that was used.\nWhile pull requests are very much welcome, please try to provide detailed docstrings for all non-trivial methods.","category":"section"},{"location":"todo/#TODO","page":"Contributing","title":"TODO","text":"Allow for non-normal uncertainties in measurements e.g. by interpolating and deriving the Kullback-Leibler divergence over a domain\nParallelism: Improve support for parallel computations of geodesics, curvature tensors and so on\nEmploy importance sampling for Monte Carlo computations\nImprove visualization capabilities for high-dimensional models\nStandardize the user-facing keyword arguments\nProvide performance benchmarks for InformationGeometry.jl\nUse IntervalArithmetic.jl and IntervalOptimisation.jl for rigorous guarantees on inference results?","category":"section"},{"location":"methodlist/#List-of-useful-methods","page":"List of useful methods","title":"List of useful methods","text":"The following lists docstrings for various important functions.\n\nOnce a DataModel object has been defined, it can subsequently be used to compute various quantities as follows:\n\nVarious geometric quantities which are intrinsic to the parameter manifold mathcalM can be computed as a result of the Fisher metric g (and subsequent choice of the Levi-Civita connection) such as the Riemann and Ricci tensors and the Ricci scalar R.\n\nFurther, studying the geodesics associated with a metric manifold can yield insights into its geometry.\n\nIn many applied settings, one often does not have a dataset of sufficient size for all parameters in the model to be \"practically identifiable\", which means that bounded confidence regions may only exist for very low confidence levels (e.g. up to 01sigma). In such cases, it is still possible to compute radial geodesics emanating from the MLE to study the geometry of the parameter space.\n\nA slightly more robust alternative to using geodesics is given by the so-called profile likelihood method. Essentially, it consists of pinning one of the parameters at particular values on a grid, while optimizing the remaining parameters to maximize the likelihood function at every step. Ultimately, one ends up with one-dimensional slices of the parameter manifold along which the likelihood decays most slowly.","category":"section"},{"location":"methodlist/#StatsAPI.loglikelihood-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"StatsAPI.loglikelihood","text":"loglikelihood(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the logarithm of the likelihood L, i.e. ell(mathrmdata    theta) coloneqq mathrmln big( L(mathrmdata    theta) big) given a DataModel and a parameter configuration theta.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.MLE-Tuple{DataModel}","page":"List of useful methods","title":"InformationGeometry.MLE","text":"MLE(DM::DataModel) -> Vector\n\nReturns the parameter configuration theta_textMLE in mathcalM which is estimated to have the highest likelihood of producing the observed data (under the assumption that the specified model captures the true relationship present in the data). For performance reasons, the maximum likelihood estimate is stored as a part of the DataModel type.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.MLEuncert-Tuple{DataModel}","page":"List of useful methods","title":"InformationGeometry.MLEuncert","text":"MLEuncert(DM::AbstractDataModel, mle::AbstractVector=MLE(DM), F::AbstractMatrix=FisherMetric(DM, mle); Safe::Bool=false, threshold::Real=1e-10, verbose::Bool=true)\n\nReturns vector of type Measurements.Measurement where the parameter uncertainties are approximated via the diagonal of the inverse Fisher metric. That is, the stated uncertainties are a linearized symmetric approximation of the true parameter uncertainties around the MLE.\n\nIf Safe=false, infinite values are only imputed for parameters which are structurally non-identifiable themselves, i.e. about which no information contained in the data. However, as a secondary effect, many more parameters than these can also exhibit flat profile likelihoods, if their effects are related to (and can be compensated for by) the degenerate parameters. Nevertheless, upon fixing or removing the purely degenerate parameters, the additional parameters whose profiles were only flat as a secondary effect become structurally identifiable. For Safe=true, infinite values are also imputed for any parameters related to the degenerate parameters, whose profiles may be secondarily affected.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.LogLikeMLE-Tuple{DataModel}","page":"List of useful methods","title":"InformationGeometry.LogLikeMLE","text":"LogLikeMLE(DM::DataModel) -> Real\n\nReturns the value of the log-likelihood ell when evaluated at the maximum likelihood estimate, i.e. ell(mathrmdata    theta_textMLE). For performance reasons, this value is stored as a part of the DataModel type.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.FisherMetric-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.FisherMetric","text":"FisherMetric(DM::DataModel, θ::AbstractVector{<:Number})\n\nComputes the Fisher metric g given a DataModel and a parameter configuration theta under the assumption that the likelihood L(mathrmdata    theta) is a multivariate normal distribution.\n\ng_ab(theta) coloneqq -int_mathcalD mathrmd^m y_mathrmdata  L(y_mathrmdata  theta)  fracpartial^2  mathrmln(L)partial theta^a  partial theta^b = -mathbbE bigg( fracpartial^2  mathrmln(L)partial theta^a  partial theta^b bigg)\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.GeometricDensity-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.GeometricDensity","text":"GeometricDensity(DM::AbstractDataModel, θ::AbstractVector) -> Real\n\nComputes the square root of the determinant of the Fisher metric sqrtmathrmdetbig(g(theta)big) at the point theta.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.ChristoffelSymbol-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.ChristoffelSymbol","text":"ChristoffelSymbol(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nChristoffelSymbol(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (12) Christoffel symbol Gamma at a point theta (i.e. the Christoffel symbol \"of the second kind\") through finite differencing of the Metric. Accurate to ≈ 3e-11. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.Riemann-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.Riemann","text":"Riemann(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nRiemann(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (13) Riemann tensor by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.Ricci-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.Ricci","text":"Ricci(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nRicci(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (02) Ricci tensor by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.RicciScalar-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.RicciScalar","text":"RicciScalar(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false) -> Real\nRicciScalar(Metric::Function, θ::AbstractVector; BigCalc::Bool=false) -> Real\n\nCalculates the Ricci scalar by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.GeodesicDistance-Tuple{DataModel, Vector{Float64}, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.GeodesicDistance","text":"GeodesicDistance(DM::DataModel, P::AbstractVector{<:Number}, Q::AbstractVector{<:Number}; tol::Real=1e-10)\nGeodesicDistance(Metric::Function, P::AbstractVector{<:Number}, Q::AbstractVector{<:Number}; tol::Real=1e-10)\n\nComputes the length of a geodesic connecting the points P and Q.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.AIC-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.AIC","text":"AIC(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the Akaike Information Criterion given a parameter configuration theta defined by mathrmAIC = 2  mathrmlength(theta) -2  ell(mathrmdata    theta). Lower values for the AIC indicate that the associated model function is more likely to be correct. For linearly parametrized models and small sample sizes, it is advisable to instead use the AICc which is more accurate.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.AICc-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.AICc","text":"AICc(DM::DataModel, θ::AbstractVector) -> Real\n\nComputes Akaike Information Criterion with an added correction term that prevents the AIC from selecting models with too many parameters (i.e. overfitting) in the case of small sample sizes. mathrmAICc = mathrmAIC + frac2mathrmlength(theta)^2 + 2 mathrmlength(theta)N - mathrmlength(theta) - 1 where N is the number of data points. Whereas AIC constitutes a first order estimate of the information loss, the AICc constitutes a second order estimate. However, this particular correction term assumes that the model is linearly parametrized.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.BIC-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.BIC","text":"BIC(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the Bayesian Information Criterion given a parameter configuration theta defined by mathrmBIC = mathrmln(N) cdot mathrmlength(theta) -2  ell(mathrmdata    theta) where N is the number of data points.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.IsLinearParameter","page":"List of useful methods","title":"InformationGeometry.IsLinearParameter","text":"IsLinearParameter(DM::DataModel, MLE::AbstractVector) -> BitVector\n\nChecks with respect to which parameters the model function model(x,θ) is linear and returns vector of booleans where true indicates linearity. This test is performed by comparing the Jacobians of the model for two random configurations theta_1 theta_2 in mathcalM column by column.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.ConfidenceRegionVolume","page":"List of useful methods","title":"InformationGeometry.ConfidenceRegionVolume","text":"ConfidenceRegionVolume(DM::AbstractDataModel, Confnum::Real; N::Int=Int(1e5), WE::Bool=true, Approx::Bool=false, kwargs...) -> Real\n\nComputes coordinate-invariant volume of confidence region associated with level Confnum via Monte Carlo by integrating the geometric density factor. For likelihoods which are particularly expensive to evaluate, Approx=true can improve the performance by approximating the confidence region via polygons.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.Pullback","page":"List of useful methods","title":"InformationGeometry.Pullback","text":"Pullback(DM::AbstractDataModel, ω::AbstractVector{<:Number}, θ::AbstractVector) -> Vector\n\nPull-back of a covector to the parameter manifold T^*mathcalM longleftarrow T^*mathcalD.\n\n\n\n\n\nPullback(DM::DataModel, G::AbstractArray{<:Number,2}, θ::AbstractVector) -> Matrix\n\nPull-back of a (0,2)-tensor G to the parameter manifold.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.Pushforward","page":"List of useful methods","title":"InformationGeometry.Pushforward","text":"Pushforward(DM::DataModel, X::AbstractVector, θ::AbstractVector) -> Vector\n\nCalculates the push-forward of a vector X from the parameter manifold to the data space TmathcalM longrightarrow TmathcalD.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.ParameterProfiles","page":"List of useful methods","title":"InformationGeometry.ParameterProfiles","text":"ParameterProfiles(DM::AbstractDataModel, Confnum::Real=2, Inds::AbstractVector{<:Int}=1:pdim(DM); adaptive::Bool=true, N::Int=31, plot::Bool=isloaded(:Plots), SaveTrajectories::Bool=true, IsCost::Bool=true, parallel::Bool=true, dof::Int=DOF(DM), kwargs...)\n\nComputes the profile likelihood for components Inds of the parameters θ in mathcalM over the given Domain. Returns a vector of matrices where the first column of the n-th matrix specifies the value of the n-th component and the second column specifies the associated confidence level of the best fit configuration conditional to the n-th component being fixed at the associated value in the first column. Confnum specifies the confidence level to which the profile should be computed if possible with Confnum=2 corresponding to 2σ, i.e. approximately 95.4%. Single profiles can be accessed via P[i], given a profile object P.\n\nThe kwarg IsCost=true can be used to skip the transformation from the likelihood values to the associated confidence level such that 2(LogLikeMLE(DM) - loglikelihood(DM, θ)) is returned in the second columns of the profiles. The trajectories followed during the reoptimization along the profile can be saved via SaveTrajectories=true. For adaptive=false the size of the domain is estimated from the inverse Fisher metric and the profile is evaluated on a fixed stepsize grid. Further kwargs can be passed to the optimization.\n\nExtended help\n\nFor visualization of the results, multiple methods are available, see e.g. PlotProfileTrajectories, PlotRelativeParameterTrajectories.\n\n\n\n\n\n","category":"type"},{"location":"methodlist/#InformationGeometry.ProfileBox","page":"List of useful methods","title":"InformationGeometry.ProfileBox","text":"ProfileBox(DM::AbstractDataModel, Fs::AbstractVector{<:AbstractInterpolation}, Confnum::Real=1.) -> HyperCube\n\nConstructs HyperCube which bounds the confidence region associated with the confidence level Confnum from the interpolated likelihood profiles.\n\n\n\n\n\nProfileBox(P::ParameterProfiles, Confnum::Real=1; Interp=DataInterpolations.QuadraticInterpolation, kwargs...)\n\nConstructs HyperCube which bounds the confidence region associated with the confidence level Confnum from the interpolated likelihood profiles.\n\n\n\n\n\nProfileBox(PV::ParameterProfilesView, Confnum::Real=1; Interp=DataInterpolations.QuadraticInterpolation, kwargs...)\n\nConstructs HyperCube which bounds the confidence region associated with the confidence level Confnum from the interpolated likelihood profiles.\n\n\n\n\n\n","category":"function"},{"location":"exporting/#Exporting","page":"Exporting","title":"Exporting","text":"For added convenience, InformationGeometry.jl already provides several methods, which can be used to export results like confidence regions or geodesics.\n\nIn particular, choosing the keyword adaptive=true samples the ODESolution objects roughly proportional to their curvature (instead of equidistant), which means that more samples are provided from tight bends than from segments that are straight, leading to more faithful representations of the confidence boundary when plotting.","category":"section"},{"location":"exporting/#InformationGeometry.SaveConfidence","page":"Exporting","title":"InformationGeometry.SaveConfidence","text":"SaveConfidence(sols::AbstractVector{<:AbstractODESolution}, N::Int=500; sigdigits::Int=7, adaptive::Bool=true) -> Matrix\nSaveConfidence(Planes::AbstractVector{<:Plane}, sols::AbstractVector{<:AbstractODESolution}, N::Int=500; sigdigits::Int=7, adaptive::Bool=true) -> Matrix\n\nReturns a Matrix of with N rows corresponding to the number of evaluations of each ODESolution in sols. The colums correspond to the various components of the evaluated solutions. E.g. for an ODESolution with 3 components, the 4. column in the Matrix corresponds to the evaluated first components of sols[2].\n\n\n\n\n\n","category":"function"},{"location":"exporting/#InformationGeometry.SaveDataSet","page":"Exporting","title":"InformationGeometry.SaveDataSet","text":"SaveDataSet(DS::DataSet; sigdigits::Int=0)\n\nReturns a DataFrame whose columns respectively constitute the x-values, y-values and standard distributions associated with the data points. For sigdigits > 0 the values are rounded to the specified number of significant digits.\n\n\n\n\n\n","category":"function"},{"location":"plotting/#Plotting-Fit-Results","page":"Useful Diagnostic Plots","title":"Plotting Fit Results","text":"Given a dataset type or DataModel, there are pre-defined recipes for the Plots.jl package so that they can be visualized via:\n\nDM = DataModel(DataSet(1:3, [4,5,6.5], [0.5,0.45,0.6]), (x,p)->(p[1]+p[2])*x + exp(p[1]-p[2]), [1.3, 0.2])\nplot(DM, MLE(DM))\n\nwhere the best fit corresponding to the MLE parameters MLE(DM) is drawn in by default. Alternatively, other parameter values may optionally be specified in the second argument.\n\nBy default, a linearized (i.e. approximate) confidence band is computed around the prediction via Gaussian error propagation of the inverse Fisher information, which provides a lower bound on the parameter uncertainty by the Cramér-Rao theorem.\n\nA desired (vector of) confidence level(s) in units of sigma can be specified via the Confnum keyword for more control over the bands, with Confnum=0 disabling the bands.\n\nplot(DM; Confnum=[1,2])\n\nThese linearized pointwise confidence bands only take into account the uncertainties in the fit parameters and therefore quantify pointwise at every input x (again, approximately for non-linearly parametrized models!) and quantify the interval in which the model prediction of the true parameters is estimated to lie with the specified confidence, given the observed data.\n\nHowever, depending on the uncertainty associated with the measurement process, future recorded data points can lie far outside the confidence bands. In contrast, to assess the range in which future validation measurements are likely to land, one can draw bands where the data uncertainty is added on top of the prediction uncertainty due to the parameters. This can be achieved via the Validation=true keyword in the plot method, where the Confnum can be controlled as before:\n\nplot(DM; Confnum=[1,2], Validation=true)\n\nIn the literature, these are often instead referred to as \"prediction bands\" instead of validation bands. However, this terminology is somewhat prone to confusion in my humble opinion which is why I personally prefer to avoid it.\n\nThe linearized confidence and validation bands are computed via the VariancePropagation and ValidationPropagation methods respectively.\n\nnote: Note\nValidation Propagation experimental and not reliable, for more exact results, compute actual ValidationProfiles.\n\nFor DataModels or simply datasets which have multiple different components (i.e. ydim > 1), the Symbol :Individual may be appended as the last argument to split the components into separate plots:\n\nDS122 = DataSet([1,2,3],[2,1,4,2,6.8,3.5],[0.5,0.5,0.45,0.45,0.55,0.6], (3,1,2))\nDM122 = DataModel(DS122, (x,p)-> [p[1]*x, p[2]*x])\nplot(DM122, MLE(DM122), :Individual; Confnum=1, Validation=false)\n\nas compared with plotting both in one:\n\nplot(DM122, MLE(DM122); Confnum=1, Validation=false)","category":"section"},{"location":"plotting/#Residuals","page":"Useful Diagnostic Plots","title":"Residuals","text":"PlotQuantiles\nResidualVsFitted\nResidualPlot","category":"section"},{"location":"plotting/#Plotting-Parameters","page":"Useful Diagnostic Plots","title":"Plotting Parameters","text":"ParameterPlot\nParameterSavingCallback\nTracePlot","category":"section"},{"location":"#InformationGeometry","page":"Getting Started","title":"InformationGeometry","text":"This is the documentation of InformationGeometry.jl, a Julia package for differential-geometric analyses of parameter inference problems.\n\n(Image: DOI)\n\nBuild Status\n(Image: appveyor) (Image: codecov)","category":"section"},{"location":"#Main-Uses","page":"Getting Started","title":"Main Uses","text":"maximum likelihood estimation\nconstruction and visualization of exact confidence regions\ncomputation of geometric quantities such as geodesics and curvature on the parameter manifold","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"As with any Julia package, InformationGeometry.jl can be added from the Julia terminal via\n\njulia> ] add InformationGeometry\n\nor alternatively by\n\njulia> using Pkg; Pkg.add(\"InformationGeometry\")","category":"section"},{"location":"#Citation","page":"Getting Started","title":"Citation","text":"If InformationGeometry.jl was helpful in your own work, please consider citing https://doi.org/10.48550/arXiv.2211.03421 and https://doi.org/10.5281/zenodo.5530660.","category":"section"}]
}
