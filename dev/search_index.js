var documenterSearchIndex = {"docs":
[{"location":"ODEmodels/#ODE-based-models","page":"ODE-based models","title":"ODE-based models","text":"","category":"section"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"Often, an explicit analytical expression for a given mathematical model is not known. Instead, the model might be defined implicitly, e.g. as the solution to a system of ordinary differential equations. Especially in fields such as systems biology, modeling in terms of (various kinds of) differential equations appears to be the norm.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"As a toy example, we will consider the well-known \"SIR model\" in the following, which groups a population into susceptible, infected and recovered subpopulations and assumes mass action kinetics with constant transmission and recovery rates to describe the growths and decays of the respective populations.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"While the DifferentialEquations.jl ecosystem offers many different ways of specifying such systems, we will use the syntax introduced by ModelingToolkit.jl since it is particularly convenient in this case.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# using InformationGeometry, OrdinaryDiffEq\n# using InformationGeometry, ModelingToolkit\n# @parameters t β γ\n# @variables S(t) I(t) R(t)\n# Dt = Differential(t)\n\n# SIReqs = [ Dt(S) ~ -β * I * S,\n#         Dt(I) ~ +β * I * S - γ * I,\n#         Dt(R) ~ +γ * I]\n\n# SIRstates = [S, I, R];    SIRparams = [β, γ]\n# @named SIRsys = ODESystem(SIReqs, t, SIRstates, SIRparams)","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"using InformationGeometry, ModelingToolkit\n@parameters t β γ\n@variables S(t) I(t) R(t)\nDt = Differential(t)\n\nSIReqs = [ Dt(S) ~ -β * I * S,\n        Dt(I) ~ +β * I * S - γ * I,\n        Dt(R) ~ +γ * I]\n\nSIRstates = [S, I, R];    SIRparams = [β, γ]\n@named SIRsys = ODESystem(SIReqs, t, SIRstates, SIRparams)","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"Here, the parameter β denotes the transmission rate of the disease and γ is the recovery rate. Note that in the symbolic scheme of ModelingToolkit.jl, the equal sign is represented via ~.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"An infection dataset which is well-known in the literature is taken from an influenza outbreak at a English boarding school in 1978. Its numerical values can be found e.g. in table 1 of this paper. As no uncertainties associated with the number of infections is given, we will assume the 1sigma uncertainties to be pm 5 as a reasonable value. Further, it is known that the total number of students at said boarding school was 763 and we will therefore assume the initial conditions to be","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# SIRinitial = [762, 1, 0.]","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"SIRinitial = [762, 1, 0.]","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"for the respective susceptible, infected and recovered subpopulations on day zero. Next, the DataSet object is constructed as:","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# days = collect(1:14)\n# infected = [3, 8, 28, 75, 221, 291, 255, 235, 190, 126, 70, 28, 12, 5]\n# SIRDS = DataSet(days, infected, 5ones(14))\n# SIRDS = InformNames(DataSet(days, infected, 5ones(14)), [\"Days\"], [\"Infected\"]) # hide","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"days = collect(1:14)\ninfected = [3, 8, 28, 75, 221, 291, 255, 235, 190, 126, 70, 28, 12, 5]\nSIRDS = InformNames(DataSet(days, infected, 5ones(14)), [\"Days\"], [\"Infected\"])","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"Finally, the DataModel associated with the SIR model and the given data is constructed by","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# SIRobservables = [2]\n# SIRDM = DataModel(SIRDS, SIRsys, SIRinitial, SIRobservables, [0.001, 0.1]; tol=1e-11)","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"SIRobservables = [2]\nSIRDM = DataModel(SIRDS, SIRsys, SIRinitial, SIRobservables, [0.001, 0.1], tol=1e-11)","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"where SIRobservables denotes the components of the ODESystem that have actually been observed in the given dataset (i.e. the second component which are the infected in this case). The optional vector [0.001, 0.1] is our initial guess for the parameters [β, γ] for the maximum likelihood estimation and the keyword tol specifies the desired accuracy of the ODE solver for all model predictions.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"tip: Tip\nInstead of specifying the observable components of an ODE system as an array, it is also possible to provide an arbitrary observation function with argument signature f(u), f(u,t) or f(u,t,θ). Similarly, (parts of) the initial conditions for the ODE system can be included as parameters of the problem and estimated from data by providing a splitter function of the form θ -> (u0, p). The first entry of the returned tuple will be used as the initial condition for the ODE system and the second argument enters into the ODEFunction itself.In this particular example, one might include the initial number of infections as a dynamical parameter via the splitter function θ -> ([763.0 - θ[1], θ[1], 0.0], θ[2:3]).","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"It is now possible to compute properties of this DataModel such as confidence regions, confidence bands, geodesics, profile likelihoods, curvature tensors and so on as with any other model.","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"sols = ConfidenceRegions(SIRDM, 1:2)\nVisualizeSols(SIRDM, sols)","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# using Plots # hide\n# sols = ConfidenceRegions(SIRDM, 1:2)\n# VisualizeSols(SIRDM, sols)\n# savefig(\"../assets/SIRsols.svg\"); nothing # hide","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"(Image: )","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"# B = ConfidenceBands(SIRDM, sols[2]) # hide\n# FittedPlot(SIRDM) # hide\n# plot!(B[:,1], B[:,3], label=\"2σ Conf. Band\", color=:orange) # hide\n# plot!(B[:,1], B[:,2], label=\"\", color=:orange) # hide\n# savefig(\"../assets/SIRBands.svg\"); nothing # hide","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"FittedPlot(SIRDM)\nConfidenceBands(SIRDM, sols[2])","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"(Image: )","category":"page"},{"location":"ODEmodels/","page":"ODE-based models","title":"ODE-based models","text":"While it visually appears as though the confidence regions are perfectly ellipsoidal and the model would therefore be linearly dependent on its parameters β and γ, this is of course not the case. The non-linearity with respect to the parameters becomes much more apparent further away from the MLE, as one can confirm e.g. via radial geodesics emanating from the MLE or the profile likelihood.","category":"page"},{"location":"basics/#Introduction-to-Information-Geometry","page":"Basics of Information Geometry","title":"Introduction to Information Geometry","text":"","category":"section"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"For a more detailed discussion of information geometry, see e.g. my Master's Thesis, this paper or this book.","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"Essentially, information geometry is a combination of the mathematical disciplines of differential geometry and probability theory. The main idea is to rephrase statistical problems in such a way that they can be given a geometric interpretation.","category":"page"},{"location":"basics/#Information-Divergences","page":"Basics of Information Geometry","title":"Information Divergences","text":"","category":"section"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"In information theory, the dissimilarity between two probability distributions p(x) and q(x) is generally quantified using so-called information divergences, which are positive-definite functionals. The most popular choice of information divergence is given by the Kullback-Leibler divergence D_textKLpq defined by","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"D_textKLpq coloneqq int mathrmd^m y  p(y)  mathrmln bigg( fracp(y)q(y) bigg) = mathbbE_p biggmathrmlnbigg( fracpq bigg) bigg","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"Intuitively, the Kullback-Leibler divergence corresponds to the relative increase in Shannon entropy (i.e. loss of information) that is incurred by approximating the distribution p(x) through q(x). In addition to its tangible information-theoretic interpretation, the Kullback-Leibler divergence has the following desirable properties:","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"reparametrization invariance with respect to the random variable over which the distributions are integrated,\napplicable between any two probability distributions with common support, e.g. a chi^2-distribution and a Poisson distribution or a normal and a Cauchy distribution.","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"On the other hand, the disadvantages of using information divergences such as the Kullback-Leibler divergence to measure the dissimilarity of distributions are:","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"they are typically not symmetric, i.e. D_textKLpq neq D_textKLqp\nthey usually do not satisfy a triangle inequality","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"wherefore they do not constitute distance functions (i.e. metric functions) on the underlying space of probability distributions.","category":"page"},{"location":"basics/#The-Fisher-Metric","page":"Basics of Information Geometry","title":"The Fisher Metric","text":"","category":"section"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"In practical applications, one is often particularly interested in spaces of probability distributions which form a single overarching family and can be parametrized using a parameter configuration theta in mathcalM where mathcalM constitutes a smooth manifold. Accordingly, any two members p(ytheta_1) and p(ytheta_2) of this family can be compared using e.g. the Kullback-Leibler divergence D_textKLbigp(theta_1)p(theta_2)big via the formula given above.","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"While the Kullback-Leibler divergence D_textKLpq does not constitute a proper distance function on mathcalM, it can be expanded in Taylor series around theta_textMLE in terms of its derivatives. The zeroth order of this expansion vanishes due to the definiteness of the Kullback-Leibler divergence (i.e. D_textKLqq = 0 for all distributions q). Similarly, the first order vanishes since the expectation of the components of the score is nil. Thus, the second order approximation of the Kullback-Leibler divergence is completely determined by its Hessian, which can be computed as","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"g_ab(theta) coloneqq biggfracpartial^2partial psi^a  partial psi^b  D_textKL bigp(ytheta)  p(ypsi) big bigg_psi = theta\n=  = -mathbbE_pbiggfracpartial^2  mathrmln(p)partial theta^a  partial theta^bbigg =  = mathbbE_pbiggfracpartial  mathrmln(p)partial theta^a fracpartial  mathrmln(p)partial theta^bbigg","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"where it was assumed that the order of the derivative operator and the integration involved in the expectation value can be interchanged.","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"The Hessian of the Kullback-Leibler divergence is typically referred to as the Fisher information matrix. Moreover, since it can be shown that the Fisher information is not only positive-definite but also exhibits the transformation behaviour associated with a (02)-tensor field, it can therefore be used as a Riemannian metric on the parameter manifold mathcalM.","category":"page"},{"location":"basics/","page":"Basics of Information Geometry","title":"Basics of Information Geometry","text":"Clearly, the Riemannian geometry induced on mathcalM by the Fisher metric is ill-equipped to faithfully capture the behaviour of the Kullback-Leibler divergence in its entirety (e.g. its asymmetry). Nevertheless, this Riemannian approximation already encodes many of the key aspects of the Kullback-Leibler divergence and additionally benefits from the versatility and maturity of the differential-geometric formalism. Therefore, the Fisher metric offers a convenient and powerful tool which can be used to study statistical problems in a coordinate invariant setting which focuses on intrinsic properties of the parameter manifold.","category":"page"},{"location":"confidence-regions/#Confidence-Regions","page":"Confidence Regions","title":"Confidence Regions","text":"","category":"section"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"One of the primary goals of InformationGeometry.jl is to enable the user to investigate the relationships between different parameters in a model in detail by determining and visualizing the exact confidence regions associated with the best fit parameters. In this context, exact refers to the fact that no simplifying assumptions are made about the shape of the confidence regions.","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"using InformationGeometry, Plots; gr() # hide\nDS = DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1]) # hide\nmodel(x::Real, θ::AbstractVector{<:Real}) = θ[1] * x + θ[2] # hide\nDM = DataModel(DS, model) # hide\n# p = FittedPlot(DM) # hide\n# savefig(p, \"../assets/CR-FittedPlot.svg\"); nothing # hide","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"DS = DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1])\nmodel(x::Real, θ::AbstractVector{<:Real}) = θ[1] * x + θ[2]\nDM = DataModel(DS, model)\nFittedPlot(DM)","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"(Image: )","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"Depending on how the parameters theta enter into the model, the shapes of confidence regions associated with the model may be distorted. For the linearly parametrized model y_textmodel(xtheta) = theta_1 cdot x + theta_2 from above, the 1 sigma and 2 sigma confidence regions form perfect ellipses around the maximum likelihood estimate as expected:","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"# sols = ConfidenceRegions(DM, 1:2; tol=1e-9) # hide\n# scatter([MLE(DM)[1]],[MLE(DM)[2]],marker=:c,label=\"MLE\") # hide\n# plot!(sols[1],vars=(1,2),label=\"1σ CR\",title=\"Confidence Regions for linearly parametrized model\", xlabel=\"θ[1]\", ylabel=\"θ[2]\") # hide\n# plot!(sols[2],vars=(1,2),label=\"2σ CR\") # hide\n# savefig(\"../assets/sols.svg\"); nothing # hide","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"sols = ConfidenceRegions(DM, 1:2; tol=1e-9)\nVisualizeSols(DM, sols)","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"(Image: )","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"For a non-linearly parametrized model such as y_textmodel(xtheta) = theta_1^3 cdot x + mathrmexp(theta_1 + theta_2) (which also produces a straight line fit!), the confidence regions are no longer ellipsoidal:","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"model2(x::Real, θ::AbstractVector{<:Real}) = θ[1]^3 * x + exp(θ[1] + θ[2])\nDM2 = DataModel(DS, model2)\nsols2 = ConfidenceRegions(DM2, 1:2; tol=1e-9)\n# scatter([MLE(DM2)[1]],[MLE(DM2)[2]],marker=:c,label=\"MLE\") # hide\n# plot!(sols2[1],vars=(1,2),label=\"1σ CR\",title=\"Confidence Regions for non-linearly parametrized model\", xlabel=\"θ[1]\", ylabel=\"θ[2]\") # hide\n# plot!(sols2[2],vars=(1,2),label=\"2σ CR\") # hide\n# savefig(\"../assets/sols2.svg\"); nothing # hide","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"model2(x::Real, θ::AbstractVector{<:Real}) = θ[1]^3 * x + exp(θ[1] + θ[2])\nDM2 = DataModel(DS, model2)\nsols2 = ConfidenceRegions(DM2, 1:2; tol=1e-9)\nVisualizeSols(DM2, sols2)","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"(Image: )","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"Specifically in the case of two-dimensional parameter spaces as shown here, the problem of finding the exact boundaries of the confidence regions is turned into a system of ordinary differential equations and subsequently solved using the DifferentialEquations.jl suite. As a result, the boundaries of the confidence regions are obtained in the form of ODESolution objects, which come equipped with elaborate interpolation methods.","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"Both finding as well as visualizing exact confidence regions for models depending on more than two parameters (i.e. mathrmdim  mathcalM  2) is more challenging from a technical perspective. For such models, it is clearly only possible to visualize three-dimensional slices of the parameter space at a time. The easiest way to achieve this is to intersect the confidence region with a family of 2D planes, in which the boundaries of the confidence region are computed using the 2D scheme.","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"The specific components of theta to be visualized can be passed as a tuple to ConfidenceRegion() via the keyword argument Dirs=(1,2,3). Also, the keyword N can be used to (approximately) control the number of planes with which the confidence region of interest is intersected.","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"DM3 = DataModel(DS, (x,θ)-> θ[1]^3 * x + exp(θ[1] + θ[2]) + θ[3] * sin(x))\n# Planes, sols3 = ConfidenceRegion(DM3, 1; tol=1e-6, Dirs=(1,2,3), N=50)\n# VisualizeSols(DM3, Planes, sols3)\n# p = VisualizeSols(InformationGeometry.ModelMappize(DM3), Planes, sols3; title=\"3D Confidence Region for non-linearly parametrized model\") # hide\n# savefig(p,\"../assets/sols3.svg\"); nothing # hide","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"DM3 = DataModel(DS, (x,θ)-> θ[1]^3 * x + exp(θ[1] + θ[2]) + θ[3] * sin(x))\nPlanes, sols3 = ConfidenceRegion(DM3, 1; tol=1e-6, Dirs=(1,2,3), N=50)\nVisualizeSols(DM3, Planes, sols3)","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"(Image: )","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"Here, only the 1sigma confidence region is shown. Given the non-linearity of the model, it is of course no surprise that the region is strongly distorted compared with a perfect ellipsoid.","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"Once the boundary of a confidence region associated with some particular level has been computed, it can be used to establish the most extreme deviations from the maximum likelihood prediction, which are possible at said confidence level. These can then be illustrated as so-called \"pointwise confidence bands\" around the best fit. For example, given the confidence boundaries of the model DM2 from above, the 2sigma confidence band can be obtained via:","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"# FittedPlot(DM2)\n# ConfidenceBands(DM2, sols2[2])\n# savefig(\"../assets/Bands.svg\"); nothing # hide","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"FittedPlot(DM2)\nConfidenceBands(DM2, sols2[2])","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"(Image: )","category":"page"},{"location":"confidence-regions/","page":"Confidence Regions","title":"Confidence Regions","text":"ConfidenceRegions(::DataModel,::Vector{Float64})\nConfidenceBands","category":"page"},{"location":"confidence-regions/#InformationGeometry.ConfidenceRegions-Tuple{DataModel, Vector{Float64}}","page":"Confidence Regions","title":"InformationGeometry.ConfidenceRegions","text":"ConfidenceRegions(DM::DataModel, Range::AbstractVector)\n\nComputes the boundaries of confidence regions for two-dimensional parameter spaces given a vector or range of confidence levels. A convenient interface which extends this to higher dimensions is currently still under development.\n\nFor example,\n\nConfidenceRegions(DM, 1:3; tol=1e-9)\n\ncomputes the 1sigma, 2sigma and 3sigma confidence regions associated with a given DataModel using a solver tolerance of 10^-9.\n\nKeyword arguments:\n\nIsConfVol = true can be used to specify the desired confidence level directly in terms of a probability p in 01 instead of in units of standard deviations sigma,\ntol can be used to quantify the tolerance with which the ODE which defines the confidence boundary is solved (default tol = 1e-9),\nmeth can be used to specify the solver algorithm (default meth = Tsit5()),\nAuto = Val(true) can be chosen to compute the derivatives of the likelihood using automatic differentiation,\nparallel = true parallelizes the computations of the separate confidence regions provided each process has access to the necessary objects,\ndof can be used to manually specify the degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"confidence-regions/#InformationGeometry.ConfidenceBands","page":"Confidence Regions","title":"InformationGeometry.ConfidenceBands","text":"ConfidenceBands(DM::DataModel, sol::AbstractODESolution, Xdomain::HyperCube; N::Int=300, plot::Bool=true) -> Matrix\n\nGiven a confidence interval sol, the pointwise confidence band around the model prediction is computed for x values in Xdomain by evaluating the model on the boundary of the confidence region.\n\n\n\n\n\n","category":"function"},{"location":"datamodels/#Providing-Datasets","page":"Providing Data and Models","title":"Providing Datasets","text":"","category":"section"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"Typically, one of the most difficult parts of any data science problem is to bring the data into a form which lends itself to the subsequent analysis. This section aims to describe the containers used by InformationGeometry.jl to store datasets and models in detail.","category":"page"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"The data itself is stored using the DataSet container.","category":"page"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"DataSet","category":"page"},{"location":"datamodels/#InformationGeometry.DataSet","page":"Providing Data and Models","title":"InformationGeometry.DataSet","text":"The DataSet type is a versatile container for storing data. Typically, it is constructed by passing it three vectors x, y, sigma where the components of sigma quantify the standard deviation associated with each y-value. Alternatively, a full covariance matrix can be supplied for the ydata instead of a vector of standard deviations. The contents of a DataSet DS can later be accessed via xdata(DS), ydata(DS), ysigma(DS).\n\nExamples:\n\nIn the simplest case, where all data points are mutually independent and have a single x-component and a single y-component each, a DataSet consisting of four points can be constructed via\n\nDataSet([1,2,3,4], [4,5,6.5,7.8], [0.5,0.45,0.6,0.8])\n\nor alternatively by\n\nusing LinearAlgebra\nDataSet([1,2,3,4], [4,5,6.5,7.8], Diagonal([0.5,0.45,0.6,0.8].^2))\n\nwhere the diagonal covariance matrix in the second line is equivalent to the vector of standard deviations supplied in the first line.\n\nFor measurements with multiple components, it is also possible to enter them as a Matrix where the columns correspond to the respective components.\n\nDataSet([0, 0.5, 1], [1 100; 2 103; 3 108], [0.5 8; 0.4 5; 0.6 10])\n\nNote that if the uncertainty matrix is square, it may be falsely interpreted as a covariance matrix instead of as the columnwise specification of standard deviations.\n\nMore generally, if a dataset consists of N points where each x-value has n many components and each y-value has m many components, this can be specified to the DataSet constructor via a tuple (Nnm) in addition to the vectors x, y and the covariance matrix. For example:\n\nX = [0.9, 1.0, 1.1, 1.9, 2.0, 2.1, 2.9, 3.0, 3.1, 3.9, 4.0, 4.1]\nY = [1.0, 5.0, 4.0, 8.0, 9.0, 13.0, 16.0, 20.0]\nCov = Diagonal([2.0, 4.0, 2.0, 4.0, 2.0, 4.0, 2.0, 4.0])\ndims = (4, 3, 2)\nDS = DataSet(X, Y, Cov, dims)\n\nIn this case, X is a vector consisting of the concatenated x-values (with 3 components each) for 4 different data points. The values of Y are the corresponding concatenated y-values (with 2 components each) of said 4 data points. Clearly, the covariance matrix must therefore be a positive-definite (m cdot N) times (m cdot N) matrix.\n\n\n\n\n\n","category":"type"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"To complete the specification of an inference problem, a model function which is assumed to be able to capture the relationship which is inherent in the data must be added.","category":"page"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"DataModel","category":"page"},{"location":"datamodels/#InformationGeometry.DataModel","page":"Providing Data and Models","title":"InformationGeometry.DataModel","text":"In addition to storing a DataSet, a DataModel also contains a function model(x,θ) and its derivative dmodel(x,θ) where x denotes the x-value of the data and θ is a vector of parameters on which the model depends. Crucially, dmodel contains the derivatives of the model with respect to the parameters θ, not the x-values. For example\n\nDS = DataSet([1,2,3,4], [4,5,6.5,7.8], [0.5,0.45,0.6,0.8])\nmodel(x::Number, θ::AbstractVector{<:Number}) = θ[1] * x + θ[2]\nDM = DataModel(DS, model)\n\nIn cases where the output of the model has more than one component (i.e. ydim > 1), it is advisable to define the model function in such a way that it outputs static vectors using StaticArrays.jl for increased performance. For ydim = 1, InformationGeometry.jl expects the model to output a number instead of a vector with one component. In contrast, the parameter configuration θ must always be supplied as a vector (even if it only has a single component).\n\nAn initial guess for the maximum likelihood parameters can optionally be passed to the DataModel as a vector via\n\nDM = DataModel(DS, model, [1.0,2.5])\n\nDuring the construction of a DataModel process which includes the search for the maximum likelihood estimate theta_textMLE, multiple tests are run. If necessary, these tests can be skipped by appending true as the last argument in the constructor:\n\nDM = DataModel(DS, model, [-Inf,π,1], true)\n\nIf a DataModel is constructed as shown in the above examples, the gradient of the model with respect to the parameters θ (i.e. its \"Jacobian\") will be calculated using automatic differentiation. Alternatively, an explicit analytic expression for the Jacobian can be specified by hand:\n\nusing StaticArrays\nfunction dmodel(x::Number, θ::AbstractVector{<:Number})\n   @SMatrix [x  1.]     # ∂(model)/∂θ₁ and ∂(model)/∂θ₂\nend\nDM = DataModel(DS, model, dmodel)\n\nThe output of the Jacobian must be a matrix whose columns correspond to the partial derivatives with respect to different components of θ and whose rows correspond to evaluations at different components of x. Again, although it is not strictly required, outputting the Jacobian in form of a static matrix is typically beneficial for the overall performance.\n\nIt is also possible to specify a (logarithmized) prior distribution on the parameter space to the DataModel constructor after the initial guess for the MLE. For example:\n\nusing Distributions\nDist = MvNormal(ones(2), [1 0; 0 3.])\nLogPriorFn(θ) = logpdf(Dist, θ)\nDM = DataModel(DS, model, [1.0,2.5], LogPriorFn)\n\nThe DataSet contained in a DataModel named DM can be accessed via Data(DM), whereas the model and its Jacobian can be used via Predictor(DM) and dPredictor(DM) respectively. The MLE and the value of the log-likelihood at the MLE are accessible via MLE(DM) and LogLikeMLE(DM). The logarithmized prior can be accessed via LogPrior(DM).\n\n\n\n\n\n","category":"type"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"\"Simple\" DataSets and DataModels can be visualized directly via plot(DM) using pre-written recipes for the Plots.jl package.","category":"page"},{"location":"datamodels/","page":"Providing Data and Models","title":"Providing Data and Models","text":"ModelMap","category":"page"},{"location":"datamodels/#InformationGeometry.ModelMap","page":"Providing Data and Models","title":"InformationGeometry.ModelMap","text":"ModelMap(Map::Function, InDomain::Union{Nothing,Function}, Domain::HyperCube)\nModelMap(Map::Function, InDomain::Function, xyp::Tuple{Int,Int,Int})\n\nA container which stores additional information about a model map, in particular its domain of validity. Map is the actual map (x,θ) -> model(x,θ). Domain is a HyperCube which allows one to roughly specify the ranges of the various parameters. For more complicated boundary constraints, scalar function InDomain can be specified, which should be strictly positive on the valid parameter domain.\n\nnote: Note\nA Bool-valued function which returns true in the valid domain also fits this description, which allows one to easily combine multiple constraints. Providing this information about the domain can be advantageous in the optimization process for complicated models.\n\n\n\n\n\n","category":"type"},{"location":"transformations/#Model-Transformations","page":"Model Transformations","title":"Model Transformations","text":"","category":"section"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"Occasionally, one might wish to perform coordinate transformations on the parameter space of a model without having to redefine the entire model as this can be a cumbersome process for complex models. For example, this might be useful in the fitting process when the allowable parameter range spans several orders of magnitude or when trying to enforce the positivity of parameters.","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"A few methods are provided to make this process more convenient. These include: LogTransform, ExpTransform, Log10Transform, Power10Transform, ReflectionTransform and ScaleTransform. These methods accept a vector of booleans as an optional second argument to restrict the application of the transformation to specific parameter components if desired. For first argument, one can either provide just a model function to obtain its transformed counterpart or alternatively supply an entire DataModel to be transformed.","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"using InformationGeometry # hide\nDM = DataModel(DataSet([1,2,3,4], [4,5,6.5,9], [0.5,0.45,0.6,1]), LinearModel)\nlogDM = LogTransform(DM)\nExpDM = ExpTransform(Power10Transform(DM, [false, true]), [true, false])\nSymbolicModel(logDM), SymbolicModel(ExpDM)","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"It is also possible to provide other differentiable functions for parameter transformations by hand using the following method:","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"Transform","category":"page"},{"location":"transformations/#InformationGeometry.Transform","page":"Model Transformations","title":"InformationGeometry.Transform","text":"Transform(DM::AbstractDataModel, F::Function, idxs=trues(pdim(DM))) -> DataModel\nTransform(model::Function, idxs, F::Function) -> Function\n\nTransforms the parameters of the model by the given scalar function F such that newmodel(x, θ) = oldmodel(x, F.(θ)). By providing idxs, one may restrict the application of the function F to specific parameter components.\n\n\n\n\n\n","category":"function"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"The provided scalar function F should be strictly monotonic to avoid problems when differentiating the model.","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"In addition to componentwise application of scalar functions to the parameters, there are also higher-dimensional transformations such as TranslationTransform, LinearTransform and their combination AffineTransform which allow for mixing between the components.","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"Lastly, the method LinearDecorrelation is a special case of AffineTransform which subtracts the MLE from the parameters and applies the cholesky decomposition (i.e. \"square root\") of the inverse Fisher metric at the best fit. This centers the confidence regions on the origin and will result in confidence boundaries which constitute concentric circles / spheres for linearly parametrized models. For models which are non-linear with respect to their parameters, the confidence boundaries of the \"linearly decorrelated\" model showcase the deviations of the confidence boundaries of the original model from ellipsoidal shape, therefore nicely illustrating the magnitude of the coordinate distortion present on the parameter space.","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"For general (differentiable) multivariable transformations on the parameter space, one can use:","category":"page"},{"location":"transformations/","page":"Model Transformations","title":"Model Transformations","text":"Embedding","category":"page"},{"location":"transformations/#InformationGeometry.Embedding","page":"Model Transformations","title":"InformationGeometry.Embedding","text":"Embedding(DM::AbstractDataModel, F::Function, start::AbstractVector; Domain::HyperCube=FullDomain(length(start))) -> DataModel\n\nTransforms a model function via newmodel(x, θ) = oldmodel(x, F(θ)) and returns the associated DataModel. An initial parameter configuration start as well as a Domain can optionally be passed to the DataModel constructor.\n\n\n\n\n\n","category":"function"},{"location":"kullback-leibler/#Kullback-Leibler-Divergences","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"","category":"section"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"Using the Distributions type provided by Distributions.jl, the KullbackLeibler method offers a convenient way of computing the Kullback-Leibler divergence between distributions. In several cases an analytical expression for the Kullback-Leibler divergence is known. These include: (univariate and multivariate) Normal, Cauchy, Exponential, Weibull and Gamma distributions.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"Furthermore, for distributions over a one-dimensional domain where no analytic result is known, KullbackLeibler rephrases the integral in terms of an ODE and employs an efficient integration scheme from the DifferentialEquations.jl suite. For multivariate distributions, Monte Carlo integration is used.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"Examples of use:","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"KullbackLeibler(Cauchy(1.,2.4), Normal(-4,0.5), HyperCube([-100,100]); tol=1e-12)\nKullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-50,50],[-50,50]]); Carlo=true, N=Int(1e8))","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"In addition, it is of course also possible to input generic functions, whose positivity and normalization should be ensured by the user.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"KullbackLeibler(::Function,::Function,::HyperCube)","category":"page"},{"location":"kullback-leibler/#InformationGeometry.KullbackLeibler-Tuple{Function, Function, HyperCube}","page":"Kullback-Leibler Divergences","title":"InformationGeometry.KullbackLeibler","text":"KullbackLeibler(p::Function,q::Function,Domain::HyperCube=HyperCube([-15,15]); tol=2e-15, N::Int=Int(3e7), Carlo::Bool=(length(Domain)!=1))\n\nComputes the Kullback-Leibler divergence between two probability distributions p and q over the Domain. If Carlo=true, this is done using a Monte Carlo Simulation with N samples. If the Domain is one-dimensional, the calculation is performed without Monte Carlo to a tolerance of ≈ tol.\n\nD_textKLpq coloneqq int mathrmd^m y  p(y)  mathrmln bigg( fracp(y)q(y) bigg)\n\n\n\n\n\n","category":"method"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"For example, the Kullback-Leibler divergence between a Cauchy distribution with mu=1 and s=2 and a normal (i.e. Gaussian) distribution with mu=-4 and sigma=12 can be calculated via:","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"using InformationGeometry # hide\nusing LinearAlgebra, Distributions\nKullbackLeibler(Cauchy(1.,2.), Normal(-4.,0.5), HyperCube([-100,100]); tol=1e-12)","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"Specifically, the keyword arguments used here numerically compute the divergence over the domain -100100 to a tolerance of approx 10^-12.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"The domain of the integral involved in the computation of the divergence is specified using the HyperCube datatype, which stores a cuboid region in N dimensions as a vector of intervals.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"HyperCube","category":"page"},{"location":"kullback-leibler/#InformationGeometry.HyperCube","page":"Kullback-Leibler Divergences","title":"InformationGeometry.HyperCube","text":"The HyperCube type is used to specify a cuboid region in the form of a cartesian product of N real intervals, thereby offering a convenient way of passing domains for integration or plotting between functions. A HyperCube object cube type has two fields: cube.L and cube.U which are two vectors which respectively store the lower and upper boundaries of the real intervals in order. Examples for constructing HyperCubes:\n\nHyperCube([[1,3],[π,2π],[-500,100]])\nHyperCube([1,π,-500],[3,2π,100])\nHyperCube([[-1,1]])\nHyperCube([-1,1])\nHyperCube(collect([-7,7.] for i in 1:3))\n\nExamples of quantities that can be computed from and operations involving a HyperCube object X:\n\nCubeVol(X)\nTranslateCube(X,v::AbstractVector)\nCubeWidths(X)\n\n\n\n\n\n","category":"type"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"Furthermore, the Kullback-Leibler divergence between multivariate distributions can be computed for example by","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"KullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-20,50],[-20,50]]); tol=1e-8)","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"using adaptive integration methods from HCubature.jl. Alternatively, Monte Carlo integration can be employed by specifying Carlo=true:","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"KullbackLeibler(MvNormal([0,2.5],diagm([1,4.])), MvTDist(1,[3,2],diagm([2.,3.])), HyperCube([[-50,50],[-50,50]]); Carlo=true, N=Int(5e6))","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"In addition, the keyword argument N now determines the number of points where the integrand is evaluated over the given domain -5050 times -5050.","category":"page"},{"location":"kullback-leibler/","page":"Kullback-Leibler Divergences","title":"Kullback-Leibler Divergences","text":"So far, importance sampling has not been implemented for the Monte Carlo integration. Instead, the domain is sampled uniformly.","category":"page"},{"location":"AdvancedData/#Advanced-Datasets","page":"Advanced Datasets","title":"Advanced Datasets","text":"","category":"section"},{"location":"AdvancedData/","page":"Advanced Datasets","title":"Advanced Datasets","text":"The following table illustrates the capabilities of the various data types implemented by InformationGeometry.jl:","category":"page"},{"location":"AdvancedData/","page":"Advanced Datasets","title":"Advanced Datasets","text":"Container allows non-Gaussian y-uncertainty allows x-uncertainty allows mixed x-y uncertainty allows missing values\nDataSet ✖ ✖ ✖ ✖\nDataSetExact ✔ ✔ ✖ ✖\nGeneralizedDataSet ✔ ✔ ✔ ✖\nCompositeDataSet ✖ ✔ ✖ ✔","category":"page"},{"location":"AdvancedData/","page":"Advanced Datasets","title":"Advanced Datasets","text":"DataSetExact\nCompositeDataSet\nGeneralizedDataSet","category":"page"},{"location":"AdvancedData/#InformationGeometry.DataSetExact","page":"Advanced Datasets","title":"InformationGeometry.DataSetExact","text":"DataSetExact(x::AbstractArray, y::AbstractArray, Σ_y::AbstractArray)\nDataSetExact(x::AbstractArray, Σ_x::AbstractArray, y::AbstractArray, Σ_y::AbstractArray)\nDataSetExact(xd::Distribution, yd::Distribution, dims::Tuple{Int,Int,Int}=(length(xd),1,1))\n\nA data container which allows for uncertainties in the independent variables, i.e. x-variables. Moreover, the observed data is stored in terms of two probability distributions over the spaces mathcalX^N and mathcalY^N respectively, which also allows for uncertainties in the observations that are non-Gaussian. For instance, the uncertainties associated with a given observation might follow a Cauchy, t-student, log-normal or some other smooth distribution.\n\nExamples:\n\nusing InformationGeometry, Distributions\nX = product_distribution([Normal(0, 1), Cauchy(2, 0.5)])\nY = MvTDist(2, [3, 8.], [1 0.5; 0.5 3])\nDataSetExact(X, Y, (2,1,1))\n\nnote: Note\nUncertainties in the independent x-variables are optional for DataSetExact, and can be set to zero by wrapping the x-data in a InformationGeometry.Dirac \"distribution\". The following illustrates numerically equivalent ways of encoding a dataset whose uncertainties in the x-variables is zero:using InformationGeometry, Distributions, LinearAlgebra\nDS1 = DataSetExact(InformationGeometry.Dirac([1,2]), MvNormal([5,6], Diagonal([0.1, 0.2].^2)))\nDS2 = DataSetExact([1,2], [5,6], [0.1, 0.2])\nDS3 = DataSet([1,2], [5,6], [0.1, 0.2])where DS1 == DS2 == DS3 will evaluate to true.\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#InformationGeometry.CompositeDataSet","page":"Advanced Datasets","title":"InformationGeometry.CompositeDataSet","text":"The CompositeDataSet type is a more elaborate (and typically less performant) container for storing data. Essentially, it splits observed data which has multiple y-components into separate data containers (e.g. of type DataSet), each of which corresponds to one of the components of the y-data. Crucially, each of the smaller data containers still shares the same \"kind\" of x-data, that is, the same xdim, units and so on, although they do not need to share the exact same particular x-data.\n\nThe main advantage of this approach is that it can be applied when there are missing y-components in some observations. A typical use case for CompositeDataSets are time series where multiple quantities are tracked but not every quantity is necessarily recorded at each time step. Example:\n\nusing DataFrames\nt = [1,2,3,4]\ny₁ = [2.5, 6, missing, 9];      y₂ = [missing, 5, 3.1, 1.4]\nσ₁ = 0.3*ones(4);               σ₂ = [missing, 0.2, 0.1, 0.5]\ndf = DataFrame([t y₁ σ₁ y₂ σ])\n\nxdim = 1;   ydim = 2\nCompositeDataSet(df, xdim, ydim; xerrs=false, stripedYs=true)\n\nThe boolean-valued keywords stripedXs and stripedYs can be used to indicate to the constructor whether the values and corresponding 1sigma uncertainties are given in alternating order, or whether the initial block of ydim many columns are the values and the second ydim many columns are the corresponding uncertainties. Also, xerrs=true can be used to indicate that the x-values also carry uncertainties. Basically all functions which can be called on other data containers such as DataSet have been specialized to also work with CompositeDataSets.\n\n\n\n\n\n","category":"type"},{"location":"AdvancedData/#InformationGeometry.GeneralizedDataSet","page":"Advanced Datasets","title":"InformationGeometry.GeneralizedDataSet","text":"GeneralizedDataSet(dist::ContinuousMultivariateDistribution, dims::Tuple{Int,Int,Int}=(length(dist), 1, 1))\n\nData structure which can take general x-y-covariance into account where dims=(Npoints, xdim, ydim) indicates the dimensionality of the data. dist should constitute a smooth distribution over the space mathcalX^N times mathcalY^N where mean(dist) is interpreted as the concatenation of the (most likely values for the) observations (x_1  x_N y_1  y_N) and the width of dist specifies the uncertainty in the signal. Typically, dist is a multivariate Gaussian but other distributions such as Cauchy or student's t-distributions are also possible. Thus, arbitrary correlations between the dependent y and independent x variables can be encoded.\n\nnote: Note\nIf there is no correlation between the x and y variables (i.e. if the offdiagonal blocks of cov(dist) are zero), it can be more performant to use the type DataSetExact to encode the given data instead.\n\n\n\n\n\n","category":"type"},{"location":"parallelization/#Parallization","page":"Parallelization","title":"Parallization","text":"","category":"section"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"Especially for cases where every single evaluation of the likelihood is computationally expensive (e.g. because the model function is highly complex and / or has to be evaluated for a very large number of data points) a lot of performance can be gained by distributing the workload between multiple threads.","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"Early on in the development of this package, the design choice was made that computations of quantities such as the likelihood should be kept local to avoid unnecessary overhead for likelihoods which are cheap. However, computations of multiple trajectories on the confidence boundary can be evaluated in parallel.","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"A prerequisite for parallel computation is that every process has access to the necessary DataModel objects. For example, this can be achieved using the @everywhere macro from Distributed.jl. Note that in this case every step involved in the definition of the DataModel, its DataSet and model function must be performed on each worker simultaneously, e.g. by wrapping all loading and construction steps in an @everywhere begin ... end environment.","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"Alternatively, it is also possible to share data between processes using packages such as ParallelDataTransfer.jl. Here, only the final DataModel needs to be sent to other workers instead of having to perform all intermediate steps (such as the maximum likelihood estimation involved in the DataModel construction) on each worker.","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"Both the functions ConfidenceRegion() and ConfidenceRegions() accept the optional keyword parallel=true to enable parallel computations of confidence boundaries. Other methods which also accept the keyword parallel=true include PlotScalar(), ProfileLikelihood() and RadialGeodesics().","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"Example:","category":"page"},{"location":"parallelization/","page":"Parallelization","title":"Parallelization","text":"using Distributed;  addprocs(4)\n@everywhere using InformationGeometry, ParallelDataTransfer\nusing Distributions, Random, BenchmarkTools\n\nRandom.seed!(123)\nX = collect(1:300);     Y = 0.02*X.^2 - 5*X .+ 10 + rand(Normal(0,5),300)\n\nDS = DataSet(X, Y, 5 .* ones(300) + 2rand(300))\nDM = DataModel(DS, (x,θ)->sinh(θ[1])*x^2 + (θ[2]+θ[3])*x + (θ[2]-θ[3]))\n\nsendto(workers(); DM=DM)\n\n@btime ConfidenceRegion(DM, 1; parallel=true, tests=false)\n@btime ConfidenceRegion(DM, 1; parallel=false, tests=false)","category":"page"},{"location":"todo/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"todo/","page":"Contributing","title":"Contributing","text":"If you encounter a bug, feel free to file an issue detailing the problem in contrast to the behaviour you were expecting. Please provide a minimal working example and make sure to specify the particular version of InformationGeometry.jl that was used.\nWhile pull requests are very much welcome, please try to provide detailed docstrings for all non-trivial methods.","category":"page"},{"location":"todo/#TODO","page":"Contributing","title":"TODO","text":"","category":"section"},{"location":"todo/","page":"Contributing","title":"Contributing","text":"Allow for non-normal uncertainties in measurements e.g. by interpolating and deriving the Kullback-Leibler divergence over a domain\nParallelism: Improve support for parallel computations of geodesics, curvature tensors and so on\nEmploy importance sampling for Monte Carlo computations\nImprove visualization capabilities for high-dimensional models\nStandardize the user-facing keyword arguments\nProvide performance benchmarks for InformationGeometry.jl\nUse IntervalArithmetic.jl and IntervalOptimisation.jl for rigorous guarantees on inference results?","category":"page"},{"location":"methodlist/#List-of-useful-methods","page":"List of useful methods","title":"List of useful methods","text":"","category":"section"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"The following lists docstrings for various important functions.","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"Once a DataModel object has been defined, it can subsequently be used to compute various quantities as follows:","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"loglikelihood(::DataModel,::Vector{Float64})\nMLE(::DataModel)\nLogLikeMLE(::DataModel)","category":"page"},{"location":"methodlist/#StatsBase.loglikelihood-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"StatsBase.loglikelihood","text":"loglikelihood(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the logarithm of the likelihood L, i.e. ell(mathrmdata    theta) coloneqq mathrmln big( L(mathrmdata    theta) big) given a DataModel and a parameter configuration theta.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.MLE-Tuple{DataModel}","page":"List of useful methods","title":"InformationGeometry.MLE","text":"MLE(DM::DataModel) -> Vector\n\nReturns the parameter configuration theta_textMLE in mathcalM which is estimated to have the highest likelihood of producing the observed data (under the assumption that the specified model captures the true relationship present in the data). For performance reasons, the maximum likelihood estimate is stored as a part of the DataModel type.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.LogLikeMLE-Tuple{DataModel}","page":"List of useful methods","title":"InformationGeometry.LogLikeMLE","text":"LogLikeMLE(DM::DataModel) -> Real\n\nReturns the value of the log-likelihood ell when evaluated at the maximum likelihood estimate, i.e. ell(mathrmdata    theta_textMLE). For performance reasons, this value is stored as a part of the DataModel type.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"Various geometric quantities which are intrinsic to the parameter manifold mathcalM can be computed as a result of the Fisher metric g (and subsequent choice of the Levi-Civita connection) such as the Riemann and Ricci tensors and the Ricci scalar R.","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"Score(::DataModel,::Vector{Float64})\nFisherMetric(::DataModel,::Vector{Float64})\nGeometricDensity(::DataModel,::Vector{Float64})\nChristoffelSymbol(::Function,::Vector{Float64})\nRiemann(::Function,::Vector{Float64})\nRicci(::Function,::Vector{Float64})\nRicciScalar(::Function,::Vector{Float64})","category":"page"},{"location":"methodlist/#InformationGeometry.Score-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.Score","text":"Score(DM::DataModel, θ::AbstractVector{<:Number}; Auto::Val=Val(false))\n\nCalculates the gradient of the log-likelihood ell with respect to a set of parameters theta. Auto=Val(true) uses automatic differentiation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.FisherMetric-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.FisherMetric","text":"FisherMetric(DM::DataModel, θ::AbstractVector{<:Number})\n\nComputes the Fisher metric g given a DataModel and a parameter configuration theta under the assumption that the likelihood L(mathrmdata    theta) is a multivariate normal distribution.\n\ng_ab(theta) coloneqq -int_mathcalD mathrmd^m y_mathrmdata  L(y_mathrmdata  theta)  fracpartial^2  mathrmln(L)partial theta^a  partial theta^b = -mathbbE bigg( fracpartial^2  mathrmln(L)partial theta^a  partial theta^b bigg)\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.GeometricDensity-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.GeometricDensity","text":"GeometricDensity(DM::AbstractDataModel, θ::AbstractVector) -> Real\n\nComputes the square root of the determinant of the Fisher metric sqrtmathrmdetbig(g(theta)big) at the point theta.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.ChristoffelSymbol-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.ChristoffelSymbol","text":"ChristoffelSymbol(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nChristoffelSymbol(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (12) Christoffel symbol Gamma at a point theta (i.e. the Christoffel symbol \"of the second kind\") through finite differencing of the Metric. Accurate to ≈ 3e-11. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.Riemann-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.Riemann","text":"Riemann(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nRiemann(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (13) Riemann tensor by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.Ricci-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.Ricci","text":"Ricci(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false)\nRicci(Metric::Function, θ::AbstractVector; BigCalc::Bool=false)\n\nCalculates the components of the (02) Ricci tensor by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.RicciScalar-Tuple{Function, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.RicciScalar","text":"RicciScalar(DM::DataModel, θ::AbstractVector; BigCalc::Bool=false) -> Real\nRicciScalar(Metric::Function, θ::AbstractVector; BigCalc::Bool=false) -> Real\n\nCalculates the Ricci scalar by finite differencing of the Metric. BigCalc=true increases accuracy through BigFloat calculation.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"Further, studying the geodesics associated with a metric manifold can yield insights into its geometry.","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"GeodesicDistance(::DataModel,::Vector{Float64},::Vector{Float64})\nAIC(::DataModel,::Vector{Float64})\nAICc(::DataModel,::Vector{Float64})\nBIC(::DataModel,::Vector{Float64})\nIsLinearParameter\nConfidenceRegionVolume\nPullback\nPushforward","category":"page"},{"location":"methodlist/#InformationGeometry.GeodesicDistance-Tuple{DataModel, Vector{Float64}, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.GeodesicDistance","text":"GeodesicDistance(DM::DataModel, P::AbstractVector{<:Number}, Q::AbstractVector{<:Number}; tol::Real=1e-10)\nGeodesicDistance(Metric::Function, P::AbstractVector{<:Number}, Q::AbstractVector{<:Number}; tol::Real=1e-10)\n\nComputes the length of a geodesic connecting the points P and Q.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.AIC-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.AIC","text":"AIC(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the Akaike Information Criterion given a parameter configuration theta defined by mathrmAIC = 2  mathrmlength(theta) -2  ell(mathrmdata    theta). Lower values for the AIC indicate that the associated model function is more likely to be correct. For linearly parametrized models and small sample sizes, it is advisable to instead use the AICc which is more accurate.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.AICc-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.AICc","text":"AICc(DM::DataModel, θ::AbstractVector) -> Real\n\nComputes Akaike Information Criterion with an added correction term that prevents the AIC from selecting models with too many parameters (i.e. overfitting) in the case of small sample sizes. mathrmAICc = mathrmAIC + frac2mathrmlength(theta)^2 + 2 mathrmlength(theta)N - mathrmlength(theta) - 1 where N is the number of data points. Whereas AIC constitutes a first order estimate of the information loss, the AICc constitutes a second order estimate. However, this particular correction term assumes that the model is linearly parametrized.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.BIC-Tuple{DataModel, Vector{Float64}}","page":"List of useful methods","title":"InformationGeometry.BIC","text":"BIC(DM::DataModel, θ::AbstractVector) -> Real\n\nCalculates the Bayesian Information Criterion given a parameter configuration theta defined by mathrmBIC = mathrmln(N) cdot mathrmlength(theta) -2  ell(mathrmdata    theta) where N is the number of data points.\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.IsLinearParameter","page":"List of useful methods","title":"InformationGeometry.IsLinearParameter","text":"IsLinearParameter(DM::DataModel) -> BitVector\n\nChecks with respect to which parameters the model function model(x,θ) is linear and returns vector of booleans where true indicates linearity. This test is performed by comparing the Jacobians of the model for two random configurations theta_1 theta_2 in mathcalM column by column.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.ConfidenceRegionVolume","page":"List of useful methods","title":"InformationGeometry.ConfidenceRegionVolume","text":"ConfidenceRegionVolume(DM::AbstractDataModel, Confnum::Real; N::Int=Int(1e5), WE::Bool=true, Approx::Bool=false, kwargs...) -> Real\n\nComputes coordinate-invariant volume of confidence region associated with level Confnum via Monte Carlo by integrating the geometric density factor. For likelihoods which are particularly expensive to evaluate, Approx=true can improve the performance by approximating the confidence region via polygons.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.Pullback","page":"List of useful methods","title":"InformationGeometry.Pullback","text":"Pullback(DM::AbstractDataModel, ω::AbstractVector{<:Number}, θ::AbstractVector) -> Vector\n\nPull-back of a covector to the parameter manifold T^*mathcalM longleftarrow T^*mathcalD.\n\n\n\n\n\nPullback(DM::DataModel, G::AbstractArray{<:Number,2}, θ::AbstractVector) -> Matrix\n\nPull-back of a (0,2)-tensor G to the parameter manifold.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.Pushforward","page":"List of useful methods","title":"InformationGeometry.Pushforward","text":"Pushforward(DM::DataModel, X::AbstractVector, θ::AbstractVector) -> Vector\n\nCalculates the push-forward of a vector X from the parameter manifold to the data space TmathcalM longrightarrow TmathcalD.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"In many applied settings, one often does not have a dataset of sufficient size for all parameters in the model to be \"practically identifiable\", which means that bounded confidence regions may only exist for very low confidence levels (e.g. up to 01sigma). In such cases, it is still possible to compute radial geodesics emanating from the MLE to study the geometry of the parameter space.","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"A slightly more robust alternative to using geodesics is given by the so-called profile likelihood method. Essentially, it consists of pinning one of the parameters at particular values on a grid, while optimizing the remaining parameters to maximize the likelihood function at every step. Ultimately, one ends up with one-dimensional slices of the parameter manifold along which the likelihood decays most slowly.","category":"page"},{"location":"methodlist/","page":"List of useful methods","title":"List of useful methods","text":"ProfileLikelihood(::DataModel,::Int64)\nInterpolatedProfiles\nProfileBox","category":"page"},{"location":"methodlist/#InformationGeometry.ProfileLikelihood-Tuple{DataModel, Int64}","page":"List of useful methods","title":"InformationGeometry.ProfileLikelihood","text":"ProfileLikelihood(DM::AbstractDataModel, Confnum::Real=2; N::Int=50, ForcePositive::Bool=false, plot::Bool=true, parallel::Bool=false, dof::Int=pdim(DM), SaveTrajectories::Bool=false) -> Vector{Matrix}\n\nComputes the profile likelihood for each component of the parameters θ in mathcalM over the given Domain. Returns a vector of N×2 matrices where the first column of the n-th matrix specifies the value of the n-th component and the second column specifies the associated confidence level of the best fit configuration conditional to the n-th component being fixed at the associated value in the first column.\n\nThe domain over which the profile likelihood is computed is not (yet) adaptively chosen. Instead the size of the domain is estimated from the inverse Fisher metric. Therefore, often has to pass higher value for Confnum to this method than the confidence level one is actually interested in, to ensure that it is still covered (if the model is even practically identifiable in the first place).\n\n\n\n\n\n","category":"method"},{"location":"methodlist/#InformationGeometry.InterpolatedProfiles","page":"List of useful methods","title":"InformationGeometry.InterpolatedProfiles","text":"InterpolatedProfiles(M::AbstractVector{<:AbstractMatrix}) -> Vector{Function}\n\nInterpolates the Vector{Matrix} output of ProfileLikelihood() with cubic splines.\n\n\n\n\n\n","category":"function"},{"location":"methodlist/#InformationGeometry.ProfileBox","page":"List of useful methods","title":"InformationGeometry.ProfileBox","text":"ProfileBox(DM::AbstractDataModel, Fs::AbstractVector{<:DataInterpolations.AbstractInterpolation}, Confnum::Real=1.) -> HyperCube\n\nConstructs HyperCube which bounds the confidence region associated with the confidence level Confnum from the interpolated likelihood profiles.\n\n\n\n\n\n","category":"function"},{"location":"exporting/#Exporting","page":"Exporting","title":"Exporting","text":"","category":"section"},{"location":"exporting/","page":"Exporting","title":"Exporting","text":"For added convenience, InformationGeometry.jl already provides several methods, which can be used to export results like confidence regions or geodesics.","category":"page"},{"location":"exporting/","page":"Exporting","title":"Exporting","text":"SaveConfidence","category":"page"},{"location":"exporting/#InformationGeometry.SaveConfidence","page":"Exporting","title":"InformationGeometry.SaveConfidence","text":"SaveConfidence(sols::AbstractVector{<:AbstractODESolution}, N::Int=500; sigdigits::Int=7, adaptive::Bool=true) -> Matrix\nSaveConfidence(Planes::AbstractVector{<:Plane}, sols::AbstractVector{<:AbstractODESolution}, N::Int=500; sigdigits::Int=7, adaptive::Bool=true) -> Matrix\n\nReturns a Matrix of with N rows corresponding to the number of evaluations of each ODESolution in sols. The colums correspond to the various components of the evaluated solutions. E.g. for an ODESolution with 3 components, the 4. column in the Matrix corresponds to the evaluated first components of sols[2].\n\n\n\n\n\n","category":"function"},{"location":"exporting/","page":"Exporting","title":"Exporting","text":"In particular, choosing the keyword adaptive=true samples the ODESolution objects roughly proportional to their curvature (instead of equidistant), which means that more samples are provided from tight bends than from segments that are straight, leading to more faithful representations of the confidence boundary when plotting.","category":"page"},{"location":"exporting/","page":"Exporting","title":"Exporting","text":"SaveDataSet","category":"page"},{"location":"exporting/#InformationGeometry.SaveDataSet","page":"Exporting","title":"InformationGeometry.SaveDataSet","text":"SaveDataSet(DS::DataSet; sigdigits::Int=0)\n\nReturns a DataFrame whose columns respectively constitute the x-values, y-values and standard distributions associated with the data points. For sigdigits > 0 the values are rounded to the specified number of significant digits.\n\n\n\n\n\n","category":"function"},{"location":"#InformationGeometry","page":"Getting Started","title":"InformationGeometry","text":"","category":"section"},{"location":"","page":"Getting Started","title":"Getting Started","text":"This is the documentation of InformationGeometry.jl, a Julia package for differential-geometric analyses of parameter inference problems.","category":"page"},{"location":"","page":"Getting Started","title":"Getting Started","text":"(Image: DOI)","category":"page"},{"location":"","page":"Getting Started","title":"Getting Started","text":"Build Status\n(Image: appveyor) (Image: codecov)","category":"page"},{"location":"#Main-Uses","page":"Getting Started","title":"Main Uses","text":"","category":"section"},{"location":"","page":"Getting Started","title":"Getting Started","text":"maximum likelihood estimation\nconstruction and visualization of exact confidence regions\nefficient calculation of Kullback-Leibler divergences\ncomputation of geometric quantities such as geodesics and curvature on the parameter manifold","category":"page"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"","page":"Getting Started","title":"Getting Started","text":"As with any Julia package, InformationGeometry.jl can be added from the Julia terminal via","category":"page"},{"location":"","page":"Getting Started","title":"Getting Started","text":"julia> ] add InformationGeometry","category":"page"},{"location":"","page":"Getting Started","title":"Getting Started","text":"or alternatively by","category":"page"},{"location":"","page":"Getting Started","title":"Getting Started","text":"julia> using Pkg; Pkg.add(\"InformationGeometry\")","category":"page"}]
}
